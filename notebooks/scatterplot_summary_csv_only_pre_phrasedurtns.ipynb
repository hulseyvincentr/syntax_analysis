{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This code takes a .json file from the TweetyBERT decoder from a bird with ONLY pre-treatment data. Then, it generates a .csv summary file that I can use to graph against the other lesion data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Reading JSON file: /Users/mirandahulsey-vincent/Documents/allPythonCode/syntax_analysis/data_inputs/Area_X_screening_phrase_durations/USA5506/USA5506_decoded_database.json\n",
      "                                  file_name        creation_date  \\\n",
      "0  USA5506_45700.49992595_2_12_13_53_12.wav  2025-02-12T13:53:38   \n",
      "1  USA5506_45700.50054645_2_12_13_54_14.wav  2025-02-12T13:54:36   \n",
      "2  USA5506_45700.50081602_2_12_13_54_41.wav  2025-02-12T13:55:00   \n",
      "\n",
      "   song_present                         syllable_onsets_offsets_ms  \\\n",
      "0          True  {'6': [[0.0, 2255.873015873016], [2655.2380952...   \n",
      "1          True  {'6': [[0.0, 1451.7460317460318]], '5': [[1451...   \n",
      "2          True  {'7': [[0.0, 790.6349206349207]], '4': [[790.6...   \n",
      "\n",
      "                    syllable_onsets_offsets_timebins Animal ID       Date  \\\n",
      "0  {'6': [[0.0, 836], [984.0, 985], [986.0, 987],...   USA5506 2025-02-12   \n",
      "1  {'6': [[0.0, 538]], '5': [[538.0, 1126]], '4':...   USA5506 2025-02-12   \n",
      "2  {'7': [[0.0, 293]], '4': [[293.0, 399], [408.0...   USA5506 2025-02-12   \n",
      "\n",
      "  Hour Minute Second  \n",
      "0   13     53     12  \n",
      "1   13     54     14  \n",
      "2   13     54     41  \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "class DecoderDataProcessor:\n",
    "    def __init__(self, json_file_path, json_date_file_path):\n",
    "        self.json_file_path = json_file_path\n",
    "        self.json_date_file_path = json_date_file_path\n",
    "        self.decoder_dataframe = self.load_data()\n",
    "        self.subdirectory_dates = self.load_subdirectory_dates()\n",
    "\n",
    "    def load_data(self):\n",
    "        print(f\"Reading JSON file: {self.json_file_path}\")\n",
    "        with open(self.json_file_path, 'r') as f:\n",
    "            decoder_data = json.load(f)['results']\n",
    "        return pd.DataFrame(decoder_data)\n",
    "\n",
    "    def load_subdirectory_dates(self):\n",
    "        with open(self.json_date_file_path, 'r') as f:\n",
    "            json_data = json.load(f)\n",
    "            subdirectory_dates = {subdir: datetime.strptime(data['subdirectory_creation_date'], \"%Y-%m-%d\").date() for subdir, data in json_data['subdirectories'].items()}\n",
    "        return subdirectory_dates\n",
    "\n",
    "    def parse_json_safe(self, s):\n",
    "        if isinstance(s, dict):\n",
    "            return s\n",
    "        if pd.isna(s):\n",
    "            return {}\n",
    "        s = s.strip().strip(\"''\")\n",
    "        s = s.replace(\"'\", '\"')\n",
    "        try:\n",
    "            return json.loads(s)\n",
    "        except json.JSONDecodeError:\n",
    "            return {}\n",
    "\n",
    "    def organize_data(self):\n",
    "        self.decoder_dataframe['syllable_onsets_offsets_ms'] = self.decoder_dataframe['syllable_onsets_offsets_ms'].apply(self.parse_json_safe)\n",
    "        self.decoder_dataframe['syllable_onsets_offsets_timebins'] = self.decoder_dataframe['syllable_onsets_offsets_timebins'].apply(self.parse_json_safe)\n",
    "        \n",
    "        self.decoder_dataframe = self.decoder_dataframe[self.decoder_dataframe['song_present'] == True].reset_index(drop=True)\n",
    "        for i, row in self.decoder_dataframe.iterrows():\n",
    "            file_name = row['file_name']\n",
    "            animal_id, date_str, hour, minute, second = self.extract_date_time(file_name)\n",
    "            self.decoder_dataframe.at[i, 'Animal ID'] = animal_id\n",
    "            self.decoder_dataframe.at[i, 'Date'] = self.apply_year_correction(date_str)\n",
    "            self.decoder_dataframe.at[i, 'Hour'] = hour\n",
    "            self.decoder_dataframe.at[i, 'Minute'] = minute\n",
    "            self.decoder_dataframe.at[i, 'Second'] = second\n",
    "        self.decoder_dataframe['Date'] = pd.to_datetime(self.decoder_dataframe['Date'], format='%Y.%m.%d', errors='coerce')\n",
    "\n",
    "    def extract_date_time(self, file_name):\n",
    "        try:\n",
    "            parts = file_name.split('_')\n",
    "            return parts[0], f\"{parts[2]}.{parts[3]}\", parts[4], parts[5], parts[6].replace('.wav', '')\n",
    "        except IndexError:\n",
    "            return None, None, None, None, None\n",
    "\n",
    "    def apply_year_correction(self, month_day):\n",
    "        if month_day is None:\n",
    "            return None\n",
    "        try:\n",
    "            month, day = month_day.split('.')\n",
    "            for subdir, date in self.subdirectory_dates.items():\n",
    "                if date.month == int(month) and date.day == int(day):\n",
    "                    return date.strftime('%Y.%m.%d')\n",
    "        except ValueError:\n",
    "            return None\n",
    "        return None\n",
    "\n",
    "# Usage\n",
    "json_file_path = '/Users/mirandahulsey-vincent/Documents/allPythonCode/syntax_analysis/data_inputs/Area_X_screening_phrase_durations/USA5506/USA5506_decoded_database.json'\n",
    "json_date_file_path = '/Users/mirandahulsey-vincent/Documents/allPythonCode/syntax_analysis/data_inputs/Area_X_screening_phrase_durations/USA5506/USA5506_Comp2_treatment_and_recording_dates.json'\n",
    "\n",
    "processor = DecoderDataProcessor(json_file_path, json_date_file_path)\n",
    "processor.organize_data()\n",
    "print(processor.decoder_dataframe.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Animal ID: USA5506\n",
      "Treatment Date: 2026-01-01\n",
      "Treatment Type: Baseline recordings only\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "# Load the JSON file and extract the treatment date\n",
    "with open(json_date_file_path, 'r') as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "#find the animal ID from the file name:\n",
    "file_name = os.path.basename(json_date_file_path)\n",
    "animal_id = file_name.split('_')[0]  # Assuming the ID is always the first part of the file name\n",
    "\n",
    "# Extract and print the treatment date and type\n",
    "treatment_date = json_data.get(\"treatment_date\", \"Treatment date not found\")\n",
    "treatment_type = json_data.get(\"treatment_type\", \"Treatment type not found\")\n",
    "\n",
    "print(f\"Animal ID: {animal_id}\")\n",
    "print(f\"Treatment Date: {treatment_date}\")\n",
    "print(f\"Treatment Type: {treatment_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique syllable labels: ['1', '10', '11', '12', '13', '15', '16', '17', '18', '2', '3', '4', '5', '6', '7', '9']\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(processor.decoder_dataframe)\n",
    "\n",
    "# Function to extract unique syllable labels from a DataFrame\n",
    "def extract_unique_syllable_labels(df):\n",
    "    unique_syllable_labels = set()\n",
    "    for syllable_dict in df['syllable_onsets_offsets_ms']:\n",
    "        if isinstance(syllable_dict, dict):\n",
    "            unique_syllable_labels.update(syllable_dict.keys())\n",
    "    return sorted(unique_syllable_labels)  # Return a sorted list of unique labels for easier viewing\n",
    "\n",
    "# Extract and print unique syllable labels\n",
    "unique_labels = extract_unique_syllable_labels(df)\n",
    "print(\"Unique syllable labels:\", unique_labels)\n",
    "pre_treatment = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-treatment Durations DataFrame:\n",
      "               1         10          11          12          13          15  \\\n",
      "0       2.698413  78.253968   89.047619   26.984127   89.047619  132.222222   \n",
      "1      24.285714   5.396825   72.857143  132.222222  223.968254  118.730159   \n",
      "2      18.888889  21.587302  107.936508   64.761905  269.841270  105.238095   \n",
      "3      72.857143   8.095238  159.206349   10.793651  294.126984  107.936508   \n",
      "4      37.777778  40.476190   97.142857  267.142857  291.428571   80.952381   \n",
      "...          ...        ...         ...         ...         ...         ...   \n",
      "12813        NaN        NaN         NaN         NaN         NaN   10.793651   \n",
      "12814        NaN        NaN         NaN         NaN         NaN   37.777778   \n",
      "12815        NaN        NaN         NaN         NaN         NaN   59.365079   \n",
      "12816        NaN        NaN         NaN         NaN         NaN   32.380952   \n",
      "12817        NaN        NaN         NaN         NaN         NaN  196.984127   \n",
      "\n",
      "              16          17         18            2            3  \\\n",
      "0       2.698413  180.793651  45.873016  2612.063492  1602.857143   \n",
      "1      99.841270  210.476190  26.984127  2474.444444  1073.968254   \n",
      "2      16.190476  215.873016  21.587302  1926.666667    10.793651   \n",
      "3      80.952381  229.365079   8.095238     8.095238  1111.746032   \n",
      "4      72.857143  250.952381  78.253968  3019.523810     2.698413   \n",
      "...          ...         ...        ...          ...          ...   \n",
      "12813        NaN         NaN        NaN          NaN          NaN   \n",
      "12814        NaN         NaN        NaN          NaN          NaN   \n",
      "12815        NaN         NaN        NaN          NaN          NaN   \n",
      "12816        NaN         NaN        NaN          NaN          NaN   \n",
      "12817        NaN         NaN        NaN          NaN          NaN   \n",
      "\n",
      "                 4            5            6            7           9  \n",
      "0       391.269841     8.095238  2255.873016   790.634921   43.174603  \n",
      "1         2.698413  1586.666667     2.698413   960.634921  628.730159  \n",
      "2        32.380952  1154.920635     2.698413  1127.936508  391.269841  \n",
      "3      2647.142857  1249.365079    51.269841  1219.682540   45.873016  \n",
      "4      1341.111111  1813.333333  1451.746032   793.333333  828.412698  \n",
      "...            ...          ...          ...          ...         ...  \n",
      "12813          NaN          NaN          NaN          NaN         NaN  \n",
      "12814          NaN          NaN          NaN          NaN         NaN  \n",
      "12815          NaN          NaN          NaN          NaN         NaN  \n",
      "12816          NaN          NaN          NaN          NaN         NaN  \n",
      "12817          NaN          NaN          NaN          NaN         NaN  \n",
      "\n",
      "[12818 rows x 16 columns]\n",
      "CSV file has been saved to your desktop.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to calculate durations for each syllable type\n",
    "def calculate_durations(df, syllable_labels):\n",
    "    durations_dict = {label: [] for label in syllable_labels}  # Initialize a dictionary to store durations for each label\n",
    "    for index, row in df.iterrows():\n",
    "        syllable_data = row['syllable_onsets_offsets_ms']\n",
    "        for label in syllable_labels:\n",
    "            if label in syllable_data:\n",
    "                for interval in syllable_data[label]:\n",
    "                    duration = interval[1] - interval[0]  # Calculate duration\n",
    "                    durations_dict[label].append(duration)  # Append duration to the corresponding list\n",
    "    return durations_dict\n",
    "\n",
    "# Calculate durations for all syllable types in the pre-treatment data\n",
    "syllable_durations_pre_treatment = calculate_durations(pre_treatment, unique_labels)\n",
    "\n",
    "# Convert the dictionary to a DataFrame for pre-treatment data\n",
    "max_length_pre = max(len(v) for v in syllable_durations_pre_treatment.values())\n",
    "df_durations_pre = pd.DataFrame({k: pd.Series(v) for k, v in syllable_durations_pre_treatment.items()}, index=range(max_length_pre))\n",
    "\n",
    "# Display the Pre-treatment Durations DataFrame\n",
    "print(\"Pre-treatment Durations DataFrame:\")\n",
    "print(df_durations_pre)\n",
    "\n",
    "# Save the pre-treatment durations DataFrame as a CSV file on the desktop\n",
    "df_durations_pre.to_csv(f'/Users/mirandahulsey-vincent/Desktop/{animal_id}pre_treatment_durations.csv', index=False)\n",
    "\n",
    "print(\"CSV file has been saved to your desktop.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-treatment Syllable Duration Statistics:\n",
      "          Mean        SEM      Median  Standard Deviation      Variance\n",
      "1    64.398997   0.944487   72.857143           43.630800  1.903647e+03\n",
      "10   32.263630   6.178715   24.285714           29.632077  8.780600e+02\n",
      "11   94.939502   2.504794   99.841270           49.275106  2.428036e+03\n",
      "12  182.055018   2.065835  180.793651          114.500199  1.311030e+04\n",
      "13  196.505610   1.067521  223.968254           88.617109  7.852992e+03\n",
      "15   79.210349   0.376390   83.650794           42.613615  1.815920e+03\n",
      "16   45.566927   1.725090   48.571429           31.574339  9.969389e+02\n",
      "17  183.596710   0.714745  196.984127           59.748593  3.569894e+03\n",
      "18   44.369528   0.939018   43.174603           28.045071  7.865260e+02\n",
      "2   670.392385  13.805977   78.253968         1408.074288  1.982673e+06\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Function to calculate statistics for each syllable type in a given DataFrame\n",
    "def calculate_statistics(df):\n",
    "    # Dictionary to hold the statistics for each syllable\n",
    "    stats_dict = {}\n",
    "    \n",
    "    # Iterate through each syllable column in the DataFrame\n",
    "    for column in df:\n",
    "        # Drop NaN values for accurate statistics\n",
    "        data = df[column].dropna()\n",
    "        \n",
    "        # Calculate required statistics\n",
    "        mean = data.mean()\n",
    "        sem = data.sem()\n",
    "        median = data.median()\n",
    "        std_dev = data.std()\n",
    "        variance = data.var()\n",
    "        \n",
    "        # Store the statistics in the dictionary\n",
    "        stats_dict[column] = {\n",
    "            'Mean': mean,\n",
    "            'SEM': sem,\n",
    "            'Median': median,\n",
    "            'Standard Deviation': std_dev,\n",
    "            'Variance': variance\n",
    "        }\n",
    "        \n",
    "    return stats_dict\n",
    "\n",
    "# Calculate statistics for pre-treatment and post-treatment DataFrames\n",
    "stats_pre_treatment = calculate_statistics(df_durations_pre)\n",
    "\n",
    "# Optionally, convert these dictionaries to DataFrames for better visualization\n",
    "df_stats_pre_treatment = pd.DataFrame(stats_pre_treatment).T\n",
    "\n",
    "# Display the statistics DataFrames\n",
    "print(\"Pre-treatment Syllable Duration Statistics:\")\n",
    "print(df_stats_pre_treatment.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file has been saved to the directory of the JSON file.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Given code: Function to calculate statistics\n",
    "def calculate_statistics(df):\n",
    "    stats_dict = {}\n",
    "    for column in df:\n",
    "        data = df[column].dropna()\n",
    "        mean = data.mean()\n",
    "        sem = data.sem()\n",
    "        median = data.median()\n",
    "        std_dev = data.std()\n",
    "        variance = data.var()\n",
    "        stats_dict[column] = {'Mean': mean, 'SEM': sem, 'Median': median, 'Standard Deviation': std_dev, 'Variance': variance}\n",
    "    return stats_dict\n",
    "\n",
    "# Calculate statistics for pre-treatment DataFrame\n",
    "stats_pre_treatment = calculate_statistics(df_durations_pre)\n",
    "df_stats_pre_treatment = pd.DataFrame(stats_pre_treatment).T\n",
    "\n",
    "# Zero vectors for post-treatment placeholders\n",
    "zero_data_mean = [0] * len(df_stats_pre_treatment)\n",
    "zero_data_median = [0] * len(df_stats_pre_treatment)\n",
    "zero_data_variance = [0] * len(df_stats_pre_treatment)\n",
    "\n",
    "# Extracting pre-treatment means and variances\n",
    "mean_pre_treatment = df_stats_pre_treatment['Mean']\n",
    "median_pre_treatment = df_stats_pre_treatment['Median']\n",
    "variance_pre_treatment = df_stats_pre_treatment['Variance']\n",
    "\n",
    "# Combining the data into a new DataFrame\n",
    "combined_data = pd.DataFrame({\n",
    "    'Mean_Pre_Treatment': mean_pre_treatment,\n",
    "    'Mean_Post_Treatment': zero_data_mean,\n",
    "    'Median_Pre_Treatment': median_pre_treatment,\n",
    "    'Median_Post_Treatment': zero_data_median,\n",
    "    'Variance_Pre_Treatment': variance_pre_treatment,\n",
    "    'Variance_Post_Treatment': zero_data_variance\n",
    "})\n",
    "\n",
    "# Adding the animal_id as the first column of the DataFrame\n",
    "combined_data.insert(0, 'Animal_ID', animal_id)\n",
    "\n",
    "# Extract the directory from the provided JSON file path\n",
    "directory = os.path.dirname(json_file_path)\n",
    "\n",
    "# Save to CSV in the same directory as the JSON file\n",
    "combined_data.to_csv(os.path.join(directory, f'{animal_id}_treatment_comparison.csv'), index=False)\n",
    "\n",
    "print(\"CSV file has been saved to the directory of the JSON file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syntax_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
