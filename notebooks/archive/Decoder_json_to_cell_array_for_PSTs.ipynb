{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This code takes in .csv outputs from the TweetyBERT decoder, then saves the data into cell arrays inside of .mat files, which can be then input into Jeff Markowitz's probabalistic suffix tree (PST). PST pipeline can be found here: https://github.com/jmarkow/pst\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note that you need to adjust line 65 of the last cell to change the folder to where the code will save the .mat files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the data from the .json files, then organize it into a dataframe. You need to update line 7 with the path to the TweetyBERT decoder .json file, and update line 8 with the path to the .json file that contains the dates of the surgery and recordings (made by running gen_bird_params_json.ipynb on the folder with all of the recordings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Reading JSON file: /Users/mirandahulsey-vincent/Documents/allPythonCode/syntax_analysis/data_inputs/Area_X_lesions/USA5337_decoded_database.json\n",
      "Total songs in JSON: 20974\n",
      "Treatment date: 2024.04.10\n",
      "First rows of organized_data_frame with updated dates:                                  file_name  song_present  \\\n",
      "0  USA5337_45370.34258490_3_19_9_30_58.wav          True   \n",
      "1   USA5337_45370.40080635_3_19_11_8_0.wav          True   \n",
      "2  USA5337_45370.40389621_3_19_11_13_9.wav          True   \n",
      "\n",
      "                          syllable_onsets_offsets_ms  \\\n",
      "0  {'20': [[0.0, 178.0952380952381], [4028.730158...   \n",
      "1  {'20': [[0.0, 129.52380952380952], [4314.76190...   \n",
      "2  {'14': [[0.0, 2571.5873015873017]], '28': [[25...   \n",
      "\n",
      "                    syllable_onsets_offsets_timebins Animal ID       Date  \\\n",
      "0  {'20': [[0.0, 66], [1493.0, 1527]], '16': [[66...   USA5337 2024-03-19   \n",
      "1  {'20': [[0.0, 48], [1599.0000000000002, 1626]]...   USA5337 2024-03-19   \n",
      "2  {'14': [[0.0, 953]], '28': [[953.0, 956]], '20...   USA5337 2024-03-19   \n",
      "\n",
      "  Hour Minute Second  \n",
      "0   09     30     58  \n",
      "1   11     08     00  \n",
      "2   11     13     09  \n",
      "Unique recording dates: ['2024.03.19' '2024.03.15' '2024.03.09' '2024.04.18' '2024.04.07'\n",
      " '2024.03.05' '2024.03.17' '2024.03.23' '2024.03.28' '2024.03.29'\n",
      " '2024.04.12' '2024.03.22' '2024.04.16' '2024.03.04' '2024.03.13'\n",
      " '2024.03.25' '2024.04.23' '2024.04.01' '2024.03.14' '2024.03.12'\n",
      " '2024.04.09' '2024.03.10' '2024.03.16' '2024.04.17' '2024.04.24'\n",
      " '2024.03.26' '2024.03.30' '2024.04.05' '2024.04.19' '2024.04.14'\n",
      " '2024.03.21' '2024.03.18' '2024.04.03' '2024.04.04' '2024.04.02'\n",
      " '2024.03.24' '2024.03.31' '2024.03.07' '2024.04.21' '2024.03.27'\n",
      " '2024.03.20' '2024.04.20' '2024.03.11' '2024.04.13' '2024.03.06'\n",
      " '2024.04.15' '2024.04.06' '2024.04.10' '2024.04.08' '2024.03.08'\n",
      " '2024.04.22']\n",
      "Unique syllable labels: ['0', '1', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '21', '22', '23', '24', '25', '26', '27', '28', '3', '4', '5', '6', '7', '8', '9']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import ast\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize and process data\n",
    "recording_file_path_name = '/Users/mirandahulsey-vincent/Documents/allPythonCode/syntax_analysis/data_inputs/Area_X_lesions/USA5337_decoded_database.json'\n",
    "creation_date_json_path = '/Users/mirandahulsey-vincent/Documents/allPythonCode/syntax_analysis/data_inputs/Area_X_lesions/USA5337_creation_data.json'\n",
    "\n",
    "class Temp:\n",
    "    def __init__(self, recording_file_path_name, creation_date_json_path):\n",
    "        # Load and process the JSON file with recording data\n",
    "        print(f\"Reading JSON file: {recording_file_path_name}\")\n",
    "        with open(recording_file_path_name, 'r') as f:\n",
    "            decoder_data = json.load(f)['results']  # Adjusted to extract the 'results' key\n",
    "\n",
    "        print(f\"Total songs in JSON: {len(decoder_data)}\")\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        decoder_dataframe = pd.DataFrame(decoder_data)\n",
    "        decoder_dataframe['syllable_onsets_offsets_ms'] = decoder_dataframe['syllable_onsets_offsets_ms'].apply(self.parse_json_safe)\n",
    "        decoder_dataframe['syllable_onsets_offsets_timebins'] = decoder_dataframe['syllable_onsets_offsets_timebins'].apply(self.parse_json_safe)\n",
    "\n",
    "        self.dataframe = decoder_dataframe\n",
    "\n",
    "        # Load and process the creation date JSON\n",
    "        with open(creation_date_json_path, 'r') as f:\n",
    "            json_data = json.load(f)\n",
    "\n",
    "        self.treatment_date = datetime.strptime(json_data['treatment_date'], \"%Y-%m-%d\").strftime(\"%Y.%m.%d\")\n",
    "        print(f\"Treatment date: {self.treatment_date}\")\n",
    "\n",
    "        self.subdirectory_dates = {\n",
    "            subdir: data['subdirectory_creation_date'] for subdir, data in json_data['subdirectories'].items()\n",
    "        }\n",
    "\n",
    "    def return_dataframe(self):\n",
    "        return self.dataframe\n",
    "\n",
    "    def get_subdirectory_dates(self):\n",
    "        return self.subdirectory_dates\n",
    "\n",
    "    def get_treatment_date(self):\n",
    "        return self.treatment_date\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_json_safe(s):\n",
    "        \"\"\"\n",
    "        Safely parse a string representation of a JSON object.\n",
    "        Handles extra quotes and converts single quotes to double quotes.\n",
    "        \"\"\"\n",
    "        if isinstance(s, dict):\n",
    "            return s\n",
    "\n",
    "        if pd.isna(s):\n",
    "            return {}\n",
    "\n",
    "        s = s.strip()\n",
    "        if s.startswith(\"''\") and s.endswith(\"''\"):\n",
    "            s = s[2:-2]\n",
    "        elif s.startswith(\"'\") and s.endswith(\"'\"):\n",
    "            s = s[1:-1]\n",
    "\n",
    "        if not s:\n",
    "            return {}\n",
    "\n",
    "        try:\n",
    "            return json.loads(s.replace(\"'\", '\"'))\n",
    "        except json.JSONDecodeError:\n",
    "            try:\n",
    "                return ast.literal_eval(s)\n",
    "            except (ValueError, SyntaxError) as e:\n",
    "                print(f\"Error parsing string: {s}\\nError: {e}\")\n",
    "                return {}\n",
    "\n",
    "# Function to extract date, time, and animal_id from the file name\n",
    "def find_recording_dates_and_times(recording_file_path_name):\n",
    "    try:\n",
    "        file_name = recording_file_path_name.split('/')[-1]\n",
    "        split_file_name_by_underscores = file_name.split('_')\n",
    "        animal_id = split_file_name_by_underscores[0]\n",
    "        month = split_file_name_by_underscores[2].zfill(2)\n",
    "        day = split_file_name_by_underscores[3].zfill(2)\n",
    "        date = f\"{month}.{day}\"\n",
    "        hour = split_file_name_by_underscores[4].zfill(2)\n",
    "        minute = split_file_name_by_underscores[5].zfill(2)\n",
    "        second = split_file_name_by_underscores[6].replace('.wav', '').zfill(2)\n",
    "        return animal_id, date, hour, minute, second\n",
    "    except IndexError:\n",
    "        print(f\"Error: Unexpected format in file name {recording_file_path_name}\")\n",
    "        return None, None, None, None, None\n",
    "\n",
    "# Function to update the date with year from the JSON file\n",
    "def update_date_with_year(row, subdirectory_dates):\n",
    "    month_day = row['Date']\n",
    "    month = month_day.split('.')[0]\n",
    "    for subdir, date in subdirectory_dates.items():\n",
    "        year, json_month, json_day = date.split('-')\n",
    "        if json_month == month:\n",
    "            return f\"{year}.{month_day}\"\n",
    "    return None\n",
    "\n",
    "# Function to create a table and filter data where song_present is True\n",
    "def make_table(input_data_frame, subdirectory_dates):\n",
    "    only_song_data = input_data_frame[input_data_frame['song_present'] == True].reset_index(drop=True)\n",
    "    num_files_with_song = only_song_data.shape[0]\n",
    "\n",
    "    organized_data_frame = only_song_data.copy()\n",
    "    organized_data_frame['Animal ID'] = [None] * num_files_with_song\n",
    "    organized_data_frame['Date'] = [None] * num_files_with_song\n",
    "    organized_data_frame['Hour'] = [None] * num_files_with_song\n",
    "    organized_data_frame['Minute'] = [None] * num_files_with_song\n",
    "    organized_data_frame['Second'] = [None] * num_files_with_song\n",
    "\n",
    "    for i, row in only_song_data.iterrows():\n",
    "        recording_file_path_name = row['file_name']\n",
    "        try:\n",
    "            animal_id, date, hour, minute, second = find_recording_dates_and_times(recording_file_path_name)\n",
    "            organized_data_frame.at[i, 'Animal ID'] = animal_id\n",
    "            organized_data_frame.at[i, 'Date'] = date\n",
    "            organized_data_frame.at[i, 'Hour'] = hour\n",
    "            organized_data_frame.at[i, 'Minute'] = minute\n",
    "            organized_data_frame.at[i, 'Second'] = second\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {recording_file_path_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "    organized_data_frame['Date'] = organized_data_frame.apply(lambda row: update_date_with_year(row, subdirectory_dates), axis=1)\n",
    "    organized_data_frame['Date'] = pd.to_datetime(organized_data_frame['Date'], format='%Y.%m.%d', errors='coerce')\n",
    "\n",
    "    print(f\"First rows of organized_data_frame with updated dates: {organized_data_frame.head(3)}\")\n",
    "    return organized_data_frame\n",
    "\n",
    "TEMP = Temp(recording_file_path_name, creation_date_json_path)\n",
    "decoder_dataframe = TEMP.return_dataframe()\n",
    "subdirectory_dates = TEMP.get_subdirectory_dates()\n",
    "treatment_date = TEMP.get_treatment_date()\n",
    "\n",
    "organized_data_frame = make_table(decoder_dataframe, subdirectory_dates)\n",
    "\n",
    "# Find the unique dates from the recording\n",
    "def find_unique_recording_dates(data_frame):\n",
    "    unique_dates = data_frame['Date'].dt.strftime('%Y.%m.%d').unique()\n",
    "    return unique_dates\n",
    "\n",
    "unique_dates = find_unique_recording_dates(organized_data_frame)\n",
    "print(\"Unique recording dates:\", unique_dates)\n",
    "\n",
    "# Extract unique syllable labels\n",
    "organized_data_frame['syllable_onsets_offsets_ms_dict'] = organized_data_frame['syllable_onsets_offsets_ms']\n",
    "unique_syllable_labels = set()\n",
    "\n",
    "for row in organized_data_frame['syllable_onsets_offsets_ms_dict']:\n",
    "    if row:\n",
    "        unique_syllable_labels.update(row.keys())\n",
    "\n",
    "unique_syllable_labels = sorted(unique_syllable_labels)\n",
    "print(\"Unique syllable labels:\", unique_syllable_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previous versions of the loading code (ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import pandas as pd\n",
    "# import ast\n",
    "# from datetime import datetime\n",
    "\n",
    "# class temp:\n",
    "#     def __init__(self):\n",
    "#         # Define the path to the JSON file\n",
    "#         recording_file_path_name = '/Users/mirandahulsey-vincent/Documents/allPythonCode/syntax_analysis/data_inputs/Area_X_lesions/USA5288_decoded_database.json'\n",
    "\n",
    "#         # Read the JSON file\n",
    "#         print(f\"Reading JSON file: {recording_file_path_name}\")\n",
    "#         with open(recording_file_path_name, 'r') as f:\n",
    "#             decoder_data = json.load(f)['results']  # Adjusted to extract the 'results' key\n",
    "\n",
    "#         # Print the total number of songs (assuming each item in 'results' is a song)\n",
    "#         print(f\"Total songs in JSON: {len(decoder_data)}\")\n",
    "\n",
    "#         # Convert to DataFrame\n",
    "#         decoder_dataframe = pd.DataFrame(decoder_data)\n",
    "#         decoder_dataframe['syllable_onsets_offsets_ms'] = decoder_dataframe['syllable_onsets_offsets_ms'].apply(self.parse_json_safe)\n",
    "#         decoder_dataframe['syllable_onsets_offsets_timebins'] = decoder_dataframe['syllable_onsets_offsets_timebins'].apply(self.parse_json_safe)\n",
    "\n",
    "#         self.dataframe = decoder_dataframe\n",
    "\n",
    "#     def return_dataframe(self):\n",
    "#         return self.dataframe\n",
    "\n",
    "\n",
    "#     def parse_json_safe(self, s):\n",
    "#             \"\"\"\n",
    "#             Safely parse a string representation of a JSON object.\n",
    "#             Handles extra quotes and converts single quotes to double quotes.\n",
    "#             \"\"\"\n",
    "#             if isinstance(s, dict):\n",
    "#                 return s  # If it's already a dictionary, return it as is\n",
    "            \n",
    "#             if pd.isna(s):\n",
    "#                 return {}\n",
    "            \n",
    "#             # Remove surrounding single quotes\n",
    "#             s = s.strip()\n",
    "#             if s.startswith(\"''\") and s.endswith(\"''\"):\n",
    "#                 s = s[2:-2]\n",
    "#             elif s.startswith(\"'\") and s.endswith(\"'\"):\n",
    "#                 s = s[1:-1]\n",
    "            \n",
    "#             if not s:\n",
    "#                 return {}\n",
    "            \n",
    "#             try:\n",
    "#                 # First, attempt to parse using json\n",
    "#                 s_json = s.replace(\"'\", '\"')\n",
    "#                 return json.loads(s_json)\n",
    "#             except json.JSONDecodeError:\n",
    "#                 try:\n",
    "#                     # If json fails, attempt using ast.literal_eval\n",
    "#                     return ast.literal_eval(s)\n",
    "#                 except (ValueError, SyntaxError) as e:\n",
    "#                     print(f\"Error parsing string: {s}\\nError: {e}\")\n",
    "#                     return {}\n",
    "    \n",
    "# TEMP = temp()\n",
    "\n",
    "# decoder_dataframe = TEMP.return_dataframe()\n",
    "\n",
    "# print(decoder_dataframe.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from datetime import datetime\n",
    "# import json\n",
    "\n",
    "# # Load the CSV and JSON creation_date data\n",
    "# save_output_to_this_file_path = '/Users/mirandahulsey-vincent/Documents/allPythonCode/syntax_analysis/data_inputs/Area_X_lesions/USA5288_decoded_database.json'\n",
    "# path_to_json_file_with_dates = '/Users/mirandahulsey-vincent/Documents/allPythonCode/syntax_analysis/data_inputs/Area_X_lesions/USA5288_creation_data.json'\n",
    "\n",
    "\n",
    "# with open(path_to_json_file_with_dates, 'r') as f:\n",
    "#     json_data = json.load(f)\n",
    "#     # Extract surgery date and format it\n",
    "# json_surgery_date = json_data['treatment_date']  # Assuming single treatment date\n",
    "# date_obj = datetime.strptime(json_surgery_date, \"%Y-%m-%d\")\n",
    "# surgery_date = date_obj.strftime(\"%Y.%m.%d\")  # Adjust to year-first format for consistency\n",
    "# print(f\"Surgery date: {surgery_date}\")\n",
    "# # Extract subdirectory creation dates from JSON data\n",
    "# subdirectory_dates = {subdir: data['subdirectory_creation_date'] for subdir, data in json_data['subdirectories'].items()}\n",
    "\n",
    "\n",
    "# # Function to extract date, time, and animal_id from the file name\n",
    "# def find_recording_dates_and_times(recording_file_path_name):\n",
    "#     try:\n",
    "#         file_name = recording_file_path_name.split('/')[-1]\n",
    "#         split_file_name_by_underscores = file_name.split('_')\n",
    "#         animal_id = split_file_name_by_underscores[0]\n",
    "#         month = split_file_name_by_underscores[2].zfill(2)\n",
    "#         day = split_file_name_by_underscores[3].zfill(2)\n",
    "#         date = f\"{month}.{day}\"\n",
    "#         hour = split_file_name_by_underscores[4].zfill(2)\n",
    "#         minute = split_file_name_by_underscores[5].zfill(2)\n",
    "#         second = split_file_name_by_underscores[6].replace('.wav', '').zfill(2)\n",
    "#         return animal_id, date, hour, minute, second\n",
    "#     except IndexError:\n",
    "#         print(f\"Error: Unexpected format in file name {recording_file_path_name}\")\n",
    "#         return None, None, None, None, None\n",
    "\n",
    "# # Function to update the date with year from the JSON file\n",
    "# def update_date_with_year(row, subdirectory_dates):\n",
    "#     month_day = row['Date']\n",
    "#     month = month_day.split('.')[0]\n",
    "#     for subdir, date in subdirectory_dates.items():\n",
    "#         year, json_month, json_day = date.split('-')\n",
    "#         if json_month == month:\n",
    "#             # Return the date in the format YYYY.MM.DD\n",
    "#             return f\"{year}.{month_day}\"\n",
    "#     return None\n",
    "\n",
    "# # Function to create a table and filter data where song_present is True\n",
    "# def make_table(input_data_frame, subdirectory_dates):\n",
    "#     only_song_data = input_data_frame[input_data_frame['song_present'] == True].reset_index(drop=True)\n",
    "#     num_files_with_song = only_song_data.shape[0]\n",
    "    \n",
    "#     organized_data_frame = only_song_data.copy()\n",
    "#     organized_data_frame['Animal ID'] = [None] * num_files_with_song\n",
    "#     organized_data_frame['Date'] = [None] * num_files_with_song\n",
    "#     organized_data_frame['Hour'] = [None] * num_files_with_song\n",
    "#     organized_data_frame['Minute'] = [None] * num_files_with_song\n",
    "#     organized_data_frame['Second'] = [None] * num_files_with_song\n",
    "\n",
    "#     for i, row in only_song_data.iterrows():\n",
    "#         recording_file_path_name = row['file_name']\n",
    "#         try:\n",
    "#             animal_id, date, hour, minute, second = find_recording_dates_and_times(recording_file_path_name)\n",
    "#             organized_data_frame.at[i, 'Animal ID'] = animal_id\n",
    "#             organized_data_frame.at[i, 'Date'] = date\n",
    "#             organized_data_frame.at[i, 'Hour'] = hour\n",
    "#             organized_data_frame.at[i, 'Minute'] = minute\n",
    "#             organized_data_frame.at[i, 'Second'] = second\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing file {recording_file_path_name}: {e}\")\n",
    "#             continue\n",
    "\n",
    "#     # Apply the year correction to the date\n",
    "#     organized_data_frame['Date'] = organized_data_frame.apply(lambda row: update_date_with_year(row, subdirectory_dates), axis=1)\n",
    "    \n",
    "#     # Convert the updated 'Date' column to a proper datetime object for further handling\n",
    "#     organized_data_frame['Date'] = pd.to_datetime(organized_data_frame['Date'], format='%Y.%m.%d', errors='coerce')\n",
    "    \n",
    "#     print(f\"First rows of organized_data_frame with updated dates: {organized_data_frame.head(3)}\")\n",
    "#     return organized_data_frame\n",
    "\n",
    "# # Generate the organized data table\n",
    "# organized_data_frame = make_table(decoder_dataframe, subdirectory_dates)\n",
    "\n",
    "# # Find the unique dates from the recording\n",
    "# def find_unique_recording_dates(data_frame):\n",
    "#     # Extract the unique values from the 'Date' column\n",
    "#     unique_dates = data_frame['Date'].dt.strftime('%Y.%m.%d').unique()  # Convert back to string for display purposes\n",
    "#     return unique_dates\n",
    "\n",
    "# # Example usage\n",
    "# unique_dates = find_unique_recording_dates(organized_data_frame)\n",
    "# print(\"Unique recording dates:\", unique_dates)\n",
    "\n",
    "# # ### Clean syllable labels formatting, then find the unique syllable labels in the dataframe\n",
    "# # def clean_and_convert_to_dict(row):\n",
    "# #     try:\n",
    "# #         # Strip leading and trailing single quotes and whitespace\n",
    "# #         row_cleaned = row.strip(\"''\").strip()\n",
    "# #         # Replace single quotes with double quotes for valid JSON syntax\n",
    "# #         row_cleaned = row_cleaned.replace(\"'\", '\"')\n",
    "# #         # Convert the cleaned string to a dictionary\n",
    "# #         return json.loads(row_cleaned)\n",
    "# #     except json.JSONDecodeError as e:\n",
    "# #         print(f\"JSON decoding failed for row: {row}\\nError: {e}\")\n",
    "# #         return None  # Return None or some default value if decoding fails\n",
    "\n",
    "# # Apply the function to all rows in the 'syllable_onsets_offsets_ms' column\n",
    "# organized_data_frame['syllable_onsets_offsets_ms_dict'] = organized_data_frame['syllable_onsets_offsets_ms']\n",
    "\n",
    "# # Now 'syllable_onsets_offsets_ms_dict' contains the cleaned and converted dictionaries\n",
    "# # Create a set to store all unique syllable labels\n",
    "# unique_syllable_labels = set()\n",
    "\n",
    "# # Iterate through each row in the 'syllable_onsets_offsets_ms_dict' column\n",
    "# for row in organized_data_frame['syllable_onsets_offsets_ms_dict']:\n",
    "#     if row:  # Check if the row is not None or empty\n",
    "#         # Add the keys (syllable labels) to the set\n",
    "#         unique_syllable_labels.update(row.keys())\n",
    "\n",
    "# # Convert the set to a sorted list (optional, for easier viewing)\n",
    "# unique_syllable_labels = sorted(unique_syllable_labels)\n",
    "\n",
    "# # Print or return the unique syllable labels\n",
    "# print(\"Unique syllable labels:\", unique_syllable_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import pandas as pd\n",
    "# import ast\n",
    "# from datetime import datetime\n",
    "\n",
    "# class Temp:\n",
    "#     def __init__(self, recording_file_path_name, creation_date_json_path):\n",
    "#         # Load and process the JSON file with recording data\n",
    "#         print(f\"Reading JSON file: {recording_file_path_name}\")\n",
    "#         with open(recording_file_path_name, 'r') as f:\n",
    "#             decoder_data = json.load(f)['results']  # Adjusted to extract the 'results' key\n",
    "\n",
    "#         print(f\"Total songs in JSON: {len(decoder_data)}\")\n",
    "\n",
    "#         # Convert to DataFrame\n",
    "#         decoder_dataframe = pd.DataFrame(decoder_data)\n",
    "#         decoder_dataframe['syllable_onsets_offsets_ms'] = decoder_dataframe['syllable_onsets_offsets_ms'].apply(self.parse_json_safe)\n",
    "#         decoder_dataframe['syllable_onsets_offsets_timebins'] = decoder_dataframe['syllable_onsets_offsets_timebins'].apply(self.parse_json_safe)\n",
    "\n",
    "#         self.dataframe = decoder_dataframe\n",
    "\n",
    "#         # Load and process the creation date JSON\n",
    "#         with open(creation_date_json_path, 'r') as f:\n",
    "#             json_data = json.load(f)\n",
    "\n",
    "#         self.treatment_date = datetime.strptime(json_data['treatment_date'], \"%Y-%m-%d\").strftime(\"%Y.%m.%d\")\n",
    "#         print(f\"Treatment date: {self.treatment_date}\")\n",
    "\n",
    "#         self.subdirectory_dates = {\n",
    "#             subdir: data['subdirectory_creation_date'] for subdir, data in json_data['subdirectories'].items()\n",
    "#         }\n",
    "\n",
    "#     def return_dataframe(self):\n",
    "#         return self.dataframe\n",
    "\n",
    "#     def get_subdirectory_dates(self):\n",
    "#         return self.subdirectory_dates\n",
    "\n",
    "#     def get_treatment_date(self):\n",
    "#         return self.treatment_date\n",
    "\n",
    "#     @staticmethod\n",
    "#     def parse_json_safe(s):\n",
    "#         \"\"\"\n",
    "#         Safely parse a string representation of a JSON object.\n",
    "#         Handles extra quotes and converts single quotes to double quotes.\n",
    "#         \"\"\"\n",
    "#         if isinstance(s, dict):\n",
    "#             return s\n",
    "\n",
    "#         if pd.isna(s):\n",
    "#             return {}\n",
    "\n",
    "#         s = s.strip()\n",
    "#         if s.startswith(\"''\") and s.endswith(\"''\"):\n",
    "#             s = s[2:-2]\n",
    "#         elif s.startswith(\"'\") and s.endswith(\"'\"):\n",
    "#             s = s[1:-1]\n",
    "\n",
    "#         if not s:\n",
    "#             return {}\n",
    "\n",
    "#         try:\n",
    "#             return json.loads(s.replace(\"'\", '\"'))\n",
    "#         except json.JSONDecodeError:\n",
    "#             try:\n",
    "#                 return ast.literal_eval(s)\n",
    "#             except (ValueError, SyntaxError) as e:\n",
    "#                 print(f\"Error parsing string: {s}\\nError: {e}\")\n",
    "#                 return {}\n",
    "\n",
    "# # Function to extract date, time, and animal_id from the file name\n",
    "# def find_recording_dates_and_times(recording_file_path_name):\n",
    "#     try:\n",
    "#         file_name = recording_file_path_name.split('/')[-1]\n",
    "#         split_file_name_by_underscores = file_name.split('_')\n",
    "#         animal_id = split_file_name_by_underscores[0]\n",
    "#         month = split_file_name_by_underscores[2].zfill(2)\n",
    "#         day = split_file_name_by_underscores[3].zfill(2)\n",
    "#         date = f\"{month}.{day}\"\n",
    "#         hour = split_file_name_by_underscores[4].zfill(2)\n",
    "#         minute = split_file_name_by_underscores[5].zfill(2)\n",
    "#         second = split_file_name_by_underscores[6].replace('.wav', '').zfill(2)\n",
    "#         return animal_id, date, hour, minute, second\n",
    "#     except IndexError:\n",
    "#         print(f\"Error: Unexpected format in file name {recording_file_path_name}\")\n",
    "#         return None, None, None, None, None\n",
    "\n",
    "# # Function to update the date with year from the JSON file\n",
    "# def update_date_with_year(row, subdirectory_dates):\n",
    "#     month_day = row['Date']\n",
    "#     month = month_day.split('.')[0]\n",
    "#     for subdir, date in subdirectory_dates.items():\n",
    "#         year, json_month, json_day = date.split('-')\n",
    "#         if json_month == month:\n",
    "#             return f\"{year}.{month_day}\"\n",
    "#     return None\n",
    "\n",
    "# # Function to create a table and filter data where song_present is True\n",
    "# def make_table(input_data_frame, subdirectory_dates):\n",
    "#     only_song_data = input_data_frame[input_data_frame['song_present'] == True].reset_index(drop=True)\n",
    "#     num_files_with_song = only_song_data.shape[0]\n",
    "\n",
    "#     organized_data_frame = only_song_data.copy()\n",
    "#     organized_data_frame['Animal ID'] = [None] * num_files_with_song\n",
    "#     organized_data_frame['Date'] = [None] * num_files_with_song\n",
    "#     organized_data_frame['Hour'] = [None] * num_files_with_song\n",
    "#     organized_data_frame['Minute'] = [None] * num_files_with_song\n",
    "#     organized_data_frame['Second'] = [None] * num_files_with_song\n",
    "\n",
    "#     for i, row in only_song_data.iterrows():\n",
    "#         recording_file_path_name = row['file_name']\n",
    "#         try:\n",
    "#             animal_id, date, hour, minute, second = find_recording_dates_and_times(recording_file_path_name)\n",
    "#             organized_data_frame.at[i, 'Animal ID'] = animal_id\n",
    "#             organized_data_frame.at[i, 'Date'] = date\n",
    "#             organized_data_frame.at[i, 'Hour'] = hour\n",
    "#             organized_data_frame.at[i, 'Minute'] = minute\n",
    "#             organized_data_frame.at[i, 'Second'] = second\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing file {recording_file_path_name}: {e}\")\n",
    "#             continue\n",
    "\n",
    "#     organized_data_frame['Date'] = organized_data_frame.apply(lambda row: update_date_with_year(row, subdirectory_dates), axis=1)\n",
    "#     organized_data_frame['Date'] = pd.to_datetime(organized_data_frame['Date'], format='%Y.%m.%d', errors='coerce')\n",
    "\n",
    "#     print(f\"First rows of organized_data_frame with updated dates: {organized_data_frame.head(3)}\")\n",
    "#     return organized_data_frame\n",
    "\n",
    "# # Initialize and process data\n",
    "# recording_file_path_name = '/Users/mirandahulsey-vincent/Documents/allPythonCode/syntax_analysis/data_inputs/Area_X_lesions/USA5288_decoded_database.json'\n",
    "# creation_date_json_path = '/Users/mirandahulsey-vincent/Documents/allPythonCode/syntax_analysis/data_inputs/Area_X_lesions/USA5288_creation_data.json'\n",
    "\n",
    "# TEMP = Temp(recording_file_path_name, creation_date_json_path)\n",
    "# decoder_dataframe = TEMP.return_dataframe()\n",
    "# subdirectory_dates = TEMP.get_subdirectory_dates()\n",
    "# treatment_date = TEMP.get_treatment_date()\n",
    "\n",
    "# organized_data_frame = make_table(decoder_dataframe, subdirectory_dates)\n",
    "\n",
    "# # Find the unique dates from the recording\n",
    "# def find_unique_recording_dates(data_frame):\n",
    "#     unique_dates = data_frame['Date'].dt.strftime('%Y.%m.%d').unique()\n",
    "#     return unique_dates\n",
    "\n",
    "# unique_dates = find_unique_recording_dates(organized_data_frame)\n",
    "# print(\"Unique recording dates:\", unique_dates)\n",
    "\n",
    "# # Extract unique syllable labels\n",
    "# organized_data_frame['syllable_onsets_offsets_ms_dict'] = organized_data_frame['syllable_onsets_offsets_ms']\n",
    "# unique_syllable_labels = set()\n",
    "\n",
    "# for row in organized_data_frame['syllable_onsets_offsets_ms_dict']:\n",
    "#     if row:\n",
    "#         unique_syllable_labels.update(row.keys())\n",
    "\n",
    "# unique_syllable_labels = sorted(unique_syllable_labels)\n",
    "# print(\"Unique syllable labels:\", unique_syllable_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the syllable order for ONE song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import re\n",
    "\n",
    "# # Assuming this is the string you've printed\n",
    "# sample_one_song = organized_data_frame['syllable_onsets_offsets_ms'][0]\n",
    "\n",
    "# # Remove the extra outer single quotes and replace single quotes with double quotes\n",
    "# cleaned_sample = sample_one_song.strip(\"''\")\n",
    "# cleaned_sample = re.sub(r\"'\", '\"', cleaned_sample)\n",
    "\n",
    "# # Now try to load it as JSON\n",
    "# try:\n",
    "#     sample_one_song_dict = json.loads(cleaned_sample)\n",
    "#     print(\"Successfully parsed JSON.\")\n",
    "# except json.JSONDecodeError as e:\n",
    "#     print(f\"JSONDecodeError: {e}\")\n",
    "\n",
    "# # Now you can proceed with your original logic\n",
    "# syllable_times = []\n",
    "\n",
    "# # Loop through each syllable label and its corresponding onset/offset pairs\n",
    "# for syllable_label, times in sample_one_song_dict.items():\n",
    "#     for onset_offset in times:\n",
    "#         syllable_times.append([syllable_label, onset_offset[0], onset_offset[1]])\n",
    "\n",
    "# # Convert the onset and offset times to floats if needed\n",
    "# syllable_times_array = np.array([[syllable_label, float(onset), float(offset)] \n",
    "#                                  for syllable_label, onset, offset in syllable_times])\n",
    "\n",
    "# # Sort the array by the onset times (second column)\n",
    "# syllable_times_array_sorted = syllable_times_array[syllable_times_array[:, 1].argsort()]\n",
    "\n",
    "# # Display the sorted result\n",
    "# syllable_times_array_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special handling of the decoder's output to convert it to a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully accessed the dictionary.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['20', '0.0', '178.0952380952381'],\n",
       "       ['21', '1270.952380952381', '1413.968253968254'],\n",
       "       ['19', '1413.968253968254', '1432.857142857143'],\n",
       "       ['5', '1432.857142857143', '2350.3174603174607'],\n",
       "       ['16', '178.0952380952381', '1270.952380952381'],\n",
       "       ['4', '2350.3174603174607', '4028.730158730159'],\n",
       "       ['20', '4028.730158730159', '4120.476190476191'],\n",
       "       ['12', '4120.476190476191', '4930.0'],\n",
       "       ['28', '4930.0', '5000.158730158731'],\n",
       "       ['26', '5000.158730158731', '5191.746031746032'],\n",
       "       ['28', '5191.746031746032', '5302.380952380952'],\n",
       "       ['26', '5302.380952380952', '5491.269841269842'],\n",
       "       ['28', '5491.269841269842', '5593.809523809524'],\n",
       "       ['26', '5593.809523809524', '5788.0952380952385'],\n",
       "       ['28', '5788.0952380952385', '5909.52380952381'],\n",
       "       ['26', '5909.52380952381', '5966.190476190476']], dtype='<U32')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming this is the first row of your DataFrame\n",
    "sample_one_song = organized_data_frame['syllable_onsets_offsets_ms'][0]\n",
    "\n",
    "# Check if sample_one_song is already a dictionary\n",
    "if isinstance(sample_one_song, dict):\n",
    "    sample_one_song_dict = sample_one_song\n",
    "    print(\"Successfully accessed the dictionary.\")\n",
    "else:\n",
    "    print(\"The sample_one_song variable is not a dictionary. Please check its format.\")\n",
    "\n",
    "# Now you can proceed with your original logic\n",
    "syllable_times = []\n",
    "\n",
    "# Loop through each syllable label and its corresponding onset/offset pairs\n",
    "for syllable_label, times in sample_one_song_dict.items():\n",
    "    for onset_offset in times:\n",
    "        syllable_times.append([syllable_label, onset_offset[0], onset_offset[1]])\n",
    "\n",
    "# Convert the onset and offset times to floats if needed\n",
    "syllable_times_array = np.array([[syllable_label, float(onset), float(offset)] \n",
    "                                 for syllable_label, onset, offset in syllable_times])\n",
    "\n",
    "# Sort the array by the onset times (second column)\n",
    "syllable_times_array_sorted = syllable_times_array[syllable_times_array[:, 1].argsort()]\n",
    "\n",
    "# Display the sorted result\n",
    "syllable_times_array_sorted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import re\n",
    "# import numpy as np\n",
    "\n",
    "# # Assuming this is the string you've printed\n",
    "# sample_one_song = organized_data_frame['syllable_onsets_offsets_ms'][0]\n",
    "\n",
    "# # Remove the extra outer single quotes and replace single quotes with double quotes\n",
    "# cleaned_sample = sample_one_song.strip(\"''\")\n",
    "# cleaned_sample = re.sub(r\"'\", '\"', cleaned_sample)\n",
    "\n",
    "# # Now try to load it as JSON\n",
    "# try:\n",
    "#     sample_one_song_dict = json.loads(cleaned_sample)\n",
    "#     print(\"Successfully parsed JSON.\")\n",
    "# except json.JSONDecodeError as e:\n",
    "#     print(f\"JSONDecodeError: {e}\")\n",
    "\n",
    "# # Now you can proceed with your original logic\n",
    "# syllable_times = []\n",
    "\n",
    "# # Loop through each syllable label and its corresponding onset/offset pairs\n",
    "# for syllable_label, times in sample_one_song_dict.items():\n",
    "#     for onset_offset in times:\n",
    "#         syllable_times.append([syllable_label, onset_offset[0], onset_offset[1]])\n",
    "\n",
    "# # Convert the onset and offset times to floats if needed\n",
    "# syllable_times_array = np.array([[syllable_label, float(onset), float(offset)] \n",
    "#                                  for syllable_label, onset, offset in syllable_times])\n",
    "\n",
    "# # Sort the array by the onset times (second column)\n",
    "# syllable_times_array_sorted = syllable_times_array[syllable_times_array[:, 1].argsort()]\n",
    "\n",
    "# # Extract only the syllable labels in order of onset times\n",
    "# syllable_order = syllable_times_array_sorted[:, 0]\n",
    "\n",
    "# # Display the syllable order array\n",
    "# syllable_order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the syllable order for each song, and add to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import re\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# # Function to generate syllable order from a single song data (syllable_onsets_offsets_ms)\n",
    "# def get_syllable_order(sample_one_song):\n",
    "#     # Clean the input and parse it as JSON\n",
    "#     cleaned_sample = sample_one_song.strip(\"''\")\n",
    "#     cleaned_sample = re.sub(r\"'\", '\"', cleaned_sample)\n",
    "    \n",
    "#     try:\n",
    "#         sample_one_song_dict = json.loads(cleaned_sample)\n",
    "#     except json.JSONDecodeError as e:\n",
    "#         print(f\"JSONDecodeError: {e}\")\n",
    "#         return None\n",
    "\n",
    "#     syllable_times = []\n",
    "    \n",
    "#     # Loop through each syllable label and its corresponding onset/offset pairs\n",
    "#     for syllable_label, times in sample_one_song_dict.items():\n",
    "#         for onset_offset in times:\n",
    "#             syllable_times.append([syllable_label, onset_offset[0], onset_offset[1]])\n",
    "\n",
    "#     # Convert the onset and offset times to floats\n",
    "#     syllable_times_array = np.array([[syllable_label, float(onset), float(offset)] \n",
    "#                                      for syllable_label, onset, offset in syllable_times])\n",
    "\n",
    "#     # Sort the array by onset times (second column)\n",
    "#     syllable_times_array_sorted = syllable_times_array[syllable_times_array[:, 1].argsort()]\n",
    "\n",
    "#     # Extract only the syllable labels in order of onset times\n",
    "#     syllable_order = syllable_times_array_sorted[:, 0]\n",
    "    \n",
    "#     return syllable_order\n",
    "\n",
    "# # Function to iteratively find syllable orders for each row in the DataFrame\n",
    "# def find_syllable_orders(organized_data_frame):\n",
    "#     syllable_orders = []\n",
    "\n",
    "#     for i, row in organized_data_frame.iterrows():\n",
    "#         sample_one_song = row['syllable_onsets_offsets_ms']\n",
    "#         syllable_order = get_syllable_order(sample_one_song)\n",
    "#         if syllable_order is not None:\n",
    "#             syllable_orders.append(syllable_order)\n",
    "#         else:\n",
    "#             syllable_orders.append([])  # In case of a parsing error, append an empty array\n",
    "\n",
    "#     # Add a new column for the syllable orders\n",
    "#     organized_data_frame['syllable_order'] = syllable_orders\n",
    "    \n",
    "#     return organized_data_frame\n",
    "\n",
    "# # Assuming you've already loaded the data and updated the table\n",
    "# organized_data_frame = make_table(decoder_dataframe, subdirectory_dates)\n",
    "\n",
    "# # Now find syllable orders for each song in the organized_data_frame\n",
    "# organized_data_frame_with_orders = find_syllable_orders(organized_data_frame)\n",
    "\n",
    "# # Output the first few rows with syllable orders\n",
    "# print(organized_data_frame_with_orders[['syllable_order']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map each syllable label to a letter instead of a number (1->A, etc. This is necessary so we can tell labels '1' and '2' apart from '12' once they all get concatonated  into a string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test this code for ONE song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import re\n",
    "# import numpy as np\n",
    "\n",
    "# # Define a function to map syllable labels to unique characters\n",
    "# def map_labels_to_characters(syllable_orders):\n",
    "#     # Get all unique syllable labels across all rows\n",
    "#     unique_labels = sorted(set(sum(syllable_orders, [])))  # Flatten the list of lists and get unique elements\n",
    "    \n",
    "#     # Create a mapping from each unique label to a unique character (uppercase, lowercase, digits)\n",
    "#     available_chars = [chr(i) for i in range(65, 91)] + [chr(i) for i in range(97, 123)] + [chr(i) for i in range(48, 58)]  # A-Z, a-z, 0-9\n",
    "#     if len(unique_labels) > len(available_chars):\n",
    "#         raise ValueError(\"Too many unique labels to map to single characters!\")\n",
    "    \n",
    "#     label_to_char = {label: available_chars[i] for i, label in enumerate(unique_labels)}\n",
    "    \n",
    "#     # Replace each label in the syllable order with its corresponding character\n",
    "#     syllable_orders_mapped = []\n",
    "#     for order in syllable_orders:\n",
    "#         mapped_order = ''.join([label_to_char[label] for label in order])\n",
    "#         syllable_orders_mapped.append(mapped_order)\n",
    "    \n",
    "#     return syllable_orders_mapped, label_to_char\n",
    "\n",
    "# # Example syllable order (as shown in the sample)\n",
    "# syllable_orders = [\n",
    "#     ['8', '21', '22', '21', '22', '21', '22', '21', '26', '23', '2', '3', '26', '5', '11', '14']\n",
    "# ]\n",
    "\n",
    "# # Map the labels and get the string representation\n",
    "# syllable_orders_mapped, label_to_char = map_labels_to_characters(syllable_orders)\n",
    "\n",
    "# # Output the mapped syllable orders and the label-to-character mapping\n",
    "# print(\"Mapped syllable order:\", syllable_orders_mapped)\n",
    "# print(\"Label-to-character mapping:\", label_to_char)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          syllable_order\n",
      "0      [20, 21, 19, 5, 16, 4, 20, 12, 28, 26, 28, 26,...\n",
      "1      [20, 21, 16, 19, 5, 4, 20, 12, 28, 26, 28, 26,...\n",
      "2                [14, 28, 20, 7, 0, 7, 0, 7, 0, 7, 0, 1]\n",
      "3                     [20, 21, 19, 5, 16, 4, 20, 12, 28]\n",
      "4      [20, 21, 19, 5, 16, 4, 3, 8, 27, 28, 27, 28, 2...\n",
      "...                                                  ...\n",
      "16791        [20, 21, 19, 5, 16, 21, 22, 20, 24, 25, 10]\n",
      "16792  [20, 3, 8, 27, 28, 27, 28, 27, 28, 27, 28, 26,...\n",
      "16793        [20, 21, 19, 5, 16, 21, 22, 20, 24, 25, 16]\n",
      "16794                [20, 21, 19, 5, 16, 22, 20, 24, 25]\n",
      "16795       [20, 21, 19, 5, 16, 4, 9, 2, 21, 22, 20, 23]\n",
      "\n",
      "[16796 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Function to generate syllable order from a single song data (syllable_onsets_offsets_ms)\n",
    "def get_syllable_order(sample_one_song):\n",
    "    # Check if sample_one_song is already a dictionary\n",
    "    if isinstance(sample_one_song, dict):\n",
    "        sample_one_song_dict = sample_one_song\n",
    "    elif isinstance(sample_one_song, str):\n",
    "        # If it's a string, attempt to clean and parse it as JSON\n",
    "        cleaned_sample = sample_one_song.strip(\"''\")\n",
    "        cleaned_sample = re.sub(r\"'\", '\"', cleaned_sample)\n",
    "        \n",
    "        try:\n",
    "            sample_one_song_dict = json.loads(cleaned_sample)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSONDecodeError: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        # If it's neither a dict nor a string, return None\n",
    "        print(f\"Unexpected data type: {type(sample_one_song)}\")\n",
    "        return None\n",
    "\n",
    "    syllable_times = []\n",
    "    \n",
    "    # Loop through each syllable label and its corresponding onset/offset pairs\n",
    "    for syllable_label, times in sample_one_song_dict.items():\n",
    "        for onset_offset in times:\n",
    "            syllable_times.append([syllable_label, onset_offset[0], onset_offset[1]])\n",
    "\n",
    "    # Convert the onset and offset times to floats\n",
    "    syllable_times_array = np.array([[syllable_label, float(onset), float(offset)] \n",
    "                                     for syllable_label, onset, offset in syllable_times])\n",
    "\n",
    "    # Sort the array by onset times (second column)\n",
    "    syllable_times_array_sorted = syllable_times_array[syllable_times_array[:, 1].argsort()]\n",
    "\n",
    "    # Extract only the syllable labels in order of onset times\n",
    "    syllable_order = syllable_times_array_sorted[:, 0]\n",
    "    \n",
    "    return syllable_order\n",
    "\n",
    "# Function to iteratively find syllable orders for each row in the DataFrame\n",
    "def find_syllable_orders(organized_data_frame):\n",
    "    syllable_orders = []\n",
    "\n",
    "    for i, row in organized_data_frame.iterrows():\n",
    "        sample_one_song = row['syllable_onsets_offsets_ms']\n",
    "        syllable_order = get_syllable_order(sample_one_song)\n",
    "        if syllable_order is not None:\n",
    "            syllable_orders.append(syllable_order)\n",
    "        else:\n",
    "            syllable_orders.append([])  # In case of a parsing error, append an empty array\n",
    "\n",
    "    # Add a new column for the syllable orders\n",
    "    organized_data_frame['syllable_order'] = syllable_orders\n",
    "    \n",
    "    return organized_data_frame\n",
    "\n",
    "# Assuming you've already loaded the data and updated the table\n",
    "# organized_data_frame = make_table(decoder_dataframe, subdirectory_dates)\n",
    "\n",
    "# Now find syllable orders for each song in the organized_data_frame\n",
    "organized_data_frame_with_orders = find_syllable_orders(organized_data_frame)\n",
    "\n",
    "# Output the first few rows with syllable orders\n",
    "print(organized_data_frame_with_orders[['syllable_order']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do this for EVERY song in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import re\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# # Function to generate syllable order from a single song data (syllable_onsets_offsets_ms)\n",
    "# def get_syllable_order(sample_one_song):\n",
    "#     # Clean the input and parse it as JSON\n",
    "#     cleaned_sample = sample_one_song.strip(\"''\")\n",
    "#     cleaned_sample = re.sub(r\"'\", '\"', cleaned_sample)\n",
    "    \n",
    "#     try:\n",
    "#         sample_one_song_dict = json.loads(cleaned_sample)\n",
    "#     except json.JSONDecodeError as e:\n",
    "#         print(f\"JSONDecodeError: {e}\")\n",
    "#         return None\n",
    "\n",
    "#     syllable_times = []\n",
    "    \n",
    "#     # Loop through each syllable label and its corresponding onset/offset pairs\n",
    "#     for syllable_label, times in sample_one_song_dict.items():\n",
    "#         for onset_offset in times:\n",
    "#             syllable_times.append([syllable_label, onset_offset[0], onset_offset[1]])\n",
    "\n",
    "#     # Convert the onset and offset times to floats\n",
    "#     syllable_times_array = np.array([[syllable_label, float(onset), float(offset)] \n",
    "#                                      for syllable_label, onset, offset in syllable_times])\n",
    "\n",
    "#     # Sort the array by onset times (second column)\n",
    "#     syllable_times_array_sorted = syllable_times_array[syllable_times_array[:, 1].argsort()]\n",
    "\n",
    "#     # Extract only the syllable labels in order of onset times\n",
    "#     syllable_order = syllable_times_array_sorted[:, 0]\n",
    "    \n",
    "#     return syllable_order\n",
    "\n",
    "# # Function to map syllable labels to unique characters\n",
    "# def map_labels_to_characters(syllable_orders):\n",
    "#     # Flatten the list of syllable orders (since syllable_orders is a list of lists)\n",
    "#     flat_syllable_orders = [label for sublist in syllable_orders for label in sublist]\n",
    "    \n",
    "#     # Get all unique syllable labels\n",
    "#     unique_labels = sorted(set(flat_syllable_orders))\n",
    "    \n",
    "#     # Create a mapping from each unique label to a unique character (uppercase, lowercase, digits)\n",
    "#     available_chars = [chr(i) for i in range(65, 91)] + [chr(i) for i in range(97, 123)] + [chr(i) for i in range(48, 58)]  # A-Z, a-z, 0-9\n",
    "#     if len(unique_labels) > len(available_chars):\n",
    "#         raise ValueError(\"Too many unique labels to map to single characters!\")\n",
    "    \n",
    "#     label_to_char = {label: available_chars[i] for i, label in enumerate(unique_labels)}\n",
    "    \n",
    "#     # Replace each label in the syllable order with its corresponding character\n",
    "#     syllable_orders_mapped = []\n",
    "#     for order in syllable_orders:\n",
    "#         mapped_order = ''.join([label_to_char[label] for label in order])\n",
    "#         syllable_orders_mapped.append(mapped_order)\n",
    "    \n",
    "#     return syllable_orders_mapped, label_to_char\n",
    "\n",
    "# # Function to iteratively find syllable orders for each row and map them\n",
    "# def find_mapped_syllable_orders(organized_data_frame):\n",
    "#     syllable_orders = []\n",
    "\n",
    "#     for i, row in organized_data_frame.iterrows():\n",
    "#         sample_one_song = row['syllable_onsets_offsets_ms']\n",
    "#         syllable_order = get_syllable_order(sample_one_song)\n",
    "#         if syllable_order is not None:\n",
    "#             syllable_orders.append(syllable_order)\n",
    "#         else:\n",
    "#             syllable_orders.append([])  # In case of a parsing error, append an empty array\n",
    "\n",
    "#     # Map the syllable orders to unique characters\n",
    "#     syllable_orders_mapped, label_to_char = map_labels_to_characters(syllable_orders)\n",
    "\n",
    "#     # Add new columns for both the raw syllable orders and the mapped syllable orders\n",
    "#     organized_data_frame['syllable_order'] = syllable_orders\n",
    "#     organized_data_frame['mapped_syllable_order'] = syllable_orders_mapped\n",
    "    \n",
    "#     return organized_data_frame, label_to_char\n",
    "\n",
    "# # Assuming you've already loaded the data and updated the table\n",
    "# organized_data_frame = make_table(decoder_dataframe, subdirectory_dates)\n",
    "\n",
    "# # Now find and map syllable orders for each song in the organized_data_frame\n",
    "# organized_data_frame_with_orders, label_to_char = find_mapped_syllable_orders(organized_data_frame)\n",
    "\n",
    "# # Output the first few rows with both syllable orders and mapped syllable orders\n",
    "# # Output only the 'syllable_order' and 'mapped_syllable_order' columns\n",
    "# print(organized_data_frame_with_orders[['syllable_order', 'mapped_syllable_order']])\n",
    "\n",
    "# print(\"Label-to-character mapping:\", label_to_char)\n",
    "\n",
    "# # Output only the 'syllable_order' and 'mapped_syllable_order' columns\n",
    "# print(organized_data_frame_with_orders[['syllable_order', 'mapped_syllable_order']].head(2))\n",
    "# print(\"Label-to-character mapping:\", label_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          syllable_order  \\\n",
      "0      [20, 21, 19, 5, 16, 4, 20, 12, 28, 26, 28, 26,...   \n",
      "1      [20, 21, 16, 19, 5, 4, 20, 12, 28, 26, 28, 26,...   \n",
      "2                [14, 28, 20, 7, 0, 7, 0, 7, 0, 7, 0, 1]   \n",
      "3                     [20, 21, 19, 5, 16, 4, 20, 12, 28]   \n",
      "4      [20, 21, 19, 5, 16, 4, 3, 8, 27, 28, 27, 28, 2...   \n",
      "...                                                  ...   \n",
      "16791        [20, 21, 19, 5, 16, 21, 22, 20, 24, 25, 10]   \n",
      "16792  [20, 3, 8, 27, 28, 27, 28, 27, 28, 27, 28, 26,...   \n",
      "16793        [20, 21, 19, 5, 16, 21, 22, 20, 24, 25, 16]   \n",
      "16794                [20, 21, 19, 5, 16, 22, 20, 24, 25]   \n",
      "16795       [20, 21, 19, 5, 16, 4, 9, 2, 21, 22, 20, 23]   \n",
      "\n",
      "                         mapped_syllable_order  \n",
      "0                             NOLYIXNEVTVTVTVT  \n",
      "1                             NOILYXNEVTVTVTVT  \n",
      "2                                 GVNaAaAaAaAB  \n",
      "3                                    NOLYIXNEV  \n",
      "4                               NOLYIXWbUVUVUa  \n",
      "...                                        ...  \n",
      "16791                              NOLYIOPNRSC  \n",
      "16792  NWbUVUVUVUVTVOTVAVBYIXNEVTVTVTVTVTVAPNZ  \n",
      "16793                              NOLYIOPNRSI  \n",
      "16794                                NOLYIPNRS  \n",
      "16795                             NOLYIXcMOPNQ  \n",
      "\n",
      "[16796 rows x 2 columns]\n",
      "Label-to-character mapping: {'0': 'A', '1': 'B', '10': 'C', '11': 'D', '12': 'E', '13': 'F', '14': 'G', '15': 'H', '16': 'I', '17': 'J', '18': 'K', '19': 'L', '2': 'M', '20': 'N', '21': 'O', '22': 'P', '23': 'Q', '24': 'R', '25': 'S', '26': 'T', '27': 'U', '28': 'V', '3': 'W', '4': 'X', '5': 'Y', '6': 'Z', '7': 'a', '8': 'b', '9': 'c'}\n",
      "                                      syllable_order mapped_syllable_order\n",
      "0  [20, 21, 19, 5, 16, 4, 20, 12, 28, 26, 28, 26,...      NOLYIXNEVTVTVTVT\n",
      "1  [20, 21, 16, 19, 5, 4, 20, 12, 28, 26, 28, 26,...      NOILYXNEVTVTVTVT\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Function to generate syllable order from a single song data (syllable_onsets_offsets_ms)\n",
    "def get_syllable_order(sample_one_song):\n",
    "    # Check if sample_one_song is already a dictionary\n",
    "    if isinstance(sample_one_song, dict):\n",
    "        sample_one_song_dict = sample_one_song\n",
    "    elif isinstance(sample_one_song, str):\n",
    "        # If it's a string, attempt to clean and parse it as JSON\n",
    "        cleaned_sample = sample_one_song.strip(\"''\")\n",
    "        cleaned_sample = re.sub(r\"'\", '\"', cleaned_sample)\n",
    "        \n",
    "        try:\n",
    "            sample_one_song_dict = json.loads(cleaned_sample)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSONDecodeError: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        # If it's neither a dict nor a string, return None\n",
    "        print(f\"Unexpected data type: {type(sample_one_song)}\")\n",
    "        return None\n",
    "\n",
    "    syllable_times = []\n",
    "    \n",
    "    # Loop through each syllable label and its corresponding onset/offset pairs\n",
    "    for syllable_label, times in sample_one_song_dict.items():\n",
    "        for onset_offset in times:\n",
    "            syllable_times.append([syllable_label, onset_offset[0], onset_offset[1]])\n",
    "\n",
    "    # Convert the onset and offset times to floats\n",
    "    syllable_times_array = np.array([[syllable_label, float(onset), float(offset)] \n",
    "                                     for syllable_label, onset, offset in syllable_times])\n",
    "\n",
    "    # Sort the array by onset times (second column)\n",
    "    syllable_times_array_sorted = syllable_times_array[syllable_times_array[:, 1].argsort()]\n",
    "\n",
    "    # Extract only the syllable labels in order of onset times\n",
    "    syllable_order = syllable_times_array_sorted[:, 0]\n",
    "    \n",
    "    return syllable_order\n",
    "\n",
    "# Function to map syllable labels to unique characters\n",
    "def map_labels_to_characters(syllable_orders):\n",
    "    # Flatten the list of syllable orders (since syllable_orders is a list of lists)\n",
    "    flat_syllable_orders = [label for sublist in syllable_orders for label in sublist]\n",
    "    \n",
    "    # Get all unique syllable labels\n",
    "    unique_labels = sorted(set(flat_syllable_orders))\n",
    "    \n",
    "    # Create a mapping from each unique label to a unique character (uppercase, lowercase, digits)\n",
    "    available_chars = [chr(i) for i in range(65, 91)] + [chr(i) for i in range(97, 123)] + [chr(i) for i in range(48, 58)]  # A-Z, a-z, 0-9\n",
    "    if len(unique_labels) > len(available_chars):\n",
    "        raise ValueError(\"Too many unique labels to map to single characters!\")\n",
    "    \n",
    "    label_to_char = {label: available_chars[i] for i, label in enumerate(unique_labels)}\n",
    "    \n",
    "    # Replace each label in the syllable order with its corresponding character\n",
    "    syllable_orders_mapped = []\n",
    "    for order in syllable_orders:\n",
    "        mapped_order = ''.join([label_to_char[label] for label in order])\n",
    "        syllable_orders_mapped.append(mapped_order)\n",
    "    \n",
    "    return syllable_orders_mapped, label_to_char\n",
    "\n",
    "# Function to iteratively find syllable orders for each row and map them\n",
    "def find_mapped_syllable_orders(organized_data_frame):\n",
    "    syllable_orders = []\n",
    "\n",
    "    for i, row in organized_data_frame.iterrows():\n",
    "        sample_one_song = row['syllable_onsets_offsets_ms']\n",
    "        syllable_order = get_syllable_order(sample_one_song)\n",
    "        if syllable_order is not None:\n",
    "            syllable_orders.append(syllable_order)\n",
    "        else:\n",
    "            syllable_orders.append([])  # In case of a parsing error, append an empty array\n",
    "\n",
    "    # Map the syllable orders to unique characters\n",
    "    syllable_orders_mapped, label_to_char = map_labels_to_characters(syllable_orders)\n",
    "\n",
    "    # Add new columns for both the raw syllable orders and the mapped syllable orders\n",
    "    organized_data_frame['syllable_order'] = syllable_orders\n",
    "    organized_data_frame['mapped_syllable_order'] = syllable_orders_mapped\n",
    "    \n",
    "    return organized_data_frame, label_to_char\n",
    "\n",
    "# Assuming you've already loaded the data and updated the table\n",
    "# Example: organized_data_frame = make_table(decoder_dataframe, subdirectory_dates)\n",
    "\n",
    "# Now find and map syllable orders for each song in the organized_data_frame\n",
    "organized_data_frame_with_orders, label_to_char = find_mapped_syllable_orders(organized_data_frame)\n",
    "\n",
    "# Output the first few rows with both syllable orders and mapped syllable orders\n",
    "# Output only the 'syllable_order' and 'mapped_syllable_order' columns\n",
    "print(organized_data_frame_with_orders[['syllable_order', 'mapped_syllable_order']])\n",
    "\n",
    "# Print label-to-character mapping\n",
    "print(\"Label-to-character mapping:\", label_to_char)\n",
    "\n",
    "# Display the first 2 rows as an example\n",
    "print(organized_data_frame_with_orders[['syllable_order', 'mapped_syllable_order']].head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, sort the data by into different groups (e.g. by days, by pre vs. post lesion group), then export them into .mat files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test: Get all the arrays from one day of recording:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Filter the DataFrame for the first day of recording\n",
    "# def get_first_day_songs(organized_data_frame):\n",
    "#     # Find the first day of recording\n",
    "#     first_day = organized_data_frame['Date'].min()\n",
    "    \n",
    "#     # Filter the DataFrame for the first day\n",
    "#     first_day_data = organized_data_frame[organized_data_frame['Date'] == first_day]\n",
    "    \n",
    "#     return first_day_data\n",
    "\n",
    "# # Generate a 1xN cell array where each cell contains the mapped syllable order for the first day\n",
    "# def generate_first_day_cell_array(organized_data_frame):\n",
    "#     # Get the filtered data for the first day\n",
    "#     first_day_data = get_first_day_songs(organized_data_frame)\n",
    "    \n",
    "#     # Extract the mapped syllable orders into a list (which acts as a cell array)\n",
    "#     mapped_syllable_orders = first_day_data['mapped_syllable_order'].tolist()\n",
    "    \n",
    "#     # Create a 1xN cell array (as a list of lists for flexibility)\n",
    "#     cell_array = [mapped_syllable_orders]\n",
    "    \n",
    "#     return cell_array\n",
    "\n",
    "# # Assuming organized_data_frame_with_orders contains the data\n",
    "# first_day_cell_array = generate_first_day_cell_array(organized_data_frame_with_orders)\n",
    "\n",
    "# # Output the result\n",
    "# print(\"1xN cell array of mapped syllable orders for the first day of recording:\")\n",
    "# print(first_day_cell_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the syllable string arrays from each day of recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Function to get the unique dates in the DataFrame\n",
    "# def get_unique_dates(organized_data_frame):\n",
    "#     return sorted(organized_data_frame['Date'].unique())\n",
    "\n",
    "# # Function to generate a cell array for each day of recording\n",
    "# def generate_cell_arrays_for_each_day(organized_data_frame):\n",
    "#     # Get all unique dates\n",
    "#     unique_dates = get_unique_dates(organized_data_frame)\n",
    "    \n",
    "#     # Iterate through each unique date\n",
    "#     for date in unique_dates:\n",
    "#         # Filter the DataFrame for the current date\n",
    "#         day_data = organized_data_frame[organized_data_frame['Date'] == date]\n",
    "        \n",
    "#         # Extract the mapped syllable orders into a list (which acts as a cell array)\n",
    "#         mapped_syllable_orders = day_data['mapped_syllable_order'].tolist()\n",
    "        \n",
    "#         # Create a 1xN cell array (as a list of lists for flexibility)\n",
    "#         cell_array = [mapped_syllable_orders]\n",
    "        \n",
    "#         # Print the date and the corresponding cell array\n",
    "#         print(f\"Recording Date: {date}\")\n",
    "#         print(\"1xN cell array of mapped syllable orders for the day:\")\n",
    "#         print(cell_array)\n",
    "#         print(\"\\n\")\n",
    "\n",
    "# # Assuming organized_data_frame_with_orders contains the data\n",
    "# generate_cell_arrays_for_each_day(organized_data_frame_with_orders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 0: Save each day's syllable cell arrays into separate .mat files (I don't recommend this, there are too few songs for PST to find a pattern from canary song, since it's so probabalistic):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import scipy.io as sio\n",
    "# import os\n",
    "\n",
    "# # Function to get the unique dates in the DataFrame\n",
    "# def get_unique_dates(organized_data_frame):\n",
    "#     return sorted(organized_data_frame['Date'].unique())\n",
    "\n",
    "# # Function to generate and save .mat files for each day's recording\n",
    "# def generate_and_save_mat_files(organized_data_frame, save_dir):\n",
    "#     # Ensure the save directory exists\n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "#     # Get all unique dates\n",
    "#     unique_dates = get_unique_dates(organized_data_frame)\n",
    "    \n",
    "#     # Get the animal ID from the first row (assuming it's consistent across the dataset)\n",
    "#     animal_id = organized_data_frame['Animal ID'].iloc[0]\n",
    "    \n",
    "#     # Iterate through each unique date\n",
    "#     for date in unique_dates:\n",
    "#         # Filter the DataFrame for the current date\n",
    "#         day_data = organized_data_frame[organized_data_frame['Date'] == date]\n",
    "        \n",
    "#         # Extract the mapped syllable orders into a list (which acts as a cell array for MATLAB)\n",
    "#         mapped_syllable_orders = day_data['mapped_syllable_order'].tolist()\n",
    "        \n",
    "#         # Prepare the data as a dictionary for saving to .mat format\n",
    "#         data_dict = {'mapped_syllable_order': mapped_syllable_orders}\n",
    "        \n",
    "#         # Generate a file name based on the animal ID and date\n",
    "#         date_formatted = date.replace('.', '_')  # Replace dots with underscores for the file name\n",
    "#         file_name = f\"{animal_id}_{date_formatted}.mat\"\n",
    "#         file_path = os.path.join(save_dir, file_name)\n",
    "        \n",
    "#         # Save the .mat file\n",
    "#         sio.savemat(file_path, data_dict)\n",
    "        \n",
    "#         # Print confirmation\n",
    "#         print(f\"Saved {file_name} to {file_path}\")\n",
    "\n",
    "# # Directory where .mat files will be saved (change this path as needed)\n",
    "# save_directory = \"/Users/mirandahulsey-vincent/Desktop/USA5288_song_cell_arrays\"\n",
    "\n",
    "# # Assuming organized_data_frame_with_orders contains the data\n",
    "# generate_and_save_mat_files(organized_data_frame_with_orders, save_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import scipy.io as sio\n",
    "# import os\n",
    "\n",
    "# # Function to get the unique dates in the DataFrame\n",
    "# def get_unique_dates(organized_data_frame):\n",
    "#     return sorted(organized_data_frame['Date'].unique())\n",
    "\n",
    "# # Function to generate and save .mat files for each day's recording\n",
    "# def generate_and_save_mat_files(organized_data_frame, save_dir):\n",
    "#     # Ensure the save directory exists\n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "#     # Get all unique dates\n",
    "#     unique_dates = get_unique_dates(organized_data_frame)\n",
    "    \n",
    "#     # Get the animal ID from the first row (assuming it's consistent across the dataset)\n",
    "#     animal_id = organized_data_frame['Animal ID'].iloc[0]\n",
    "    \n",
    "#     # Iterate through each unique date\n",
    "#     for date in unique_dates:\n",
    "#         # Filter the DataFrame for the current date\n",
    "#         day_data = organized_data_frame[organized_data_frame['Date'] == date]\n",
    "        \n",
    "#         # Extract the mapped syllable orders and store them as a cell array (list of lists)\n",
    "#         mapped_syllable_orders = day_data['mapped_syllable_order'].tolist()\n",
    "        \n",
    "#         # Ensure the mapped_syllable_orders are formatted as a MATLAB cell array (list of cells in Python)\n",
    "#         mapped_syllable_orders_cell = [[order] for order in mapped_syllable_orders]  # Wrap each string in a list\n",
    "        \n",
    "#         # Prepare the data as a dictionary for saving to .mat format\n",
    "#         data_dict = {'mapped_syllable_order': mapped_syllable_orders_cell}\n",
    "        \n",
    "#         # Generate a file name based on the animal ID and date\n",
    "#         date_formatted = date.replace('.', '_')  # Replace dots with underscores for the file name\n",
    "#         file_name = f\"{animal_id}_{date_formatted}.mat\"\n",
    "#         file_path = os.path.join(save_dir, file_name)\n",
    "        \n",
    "#         # Save the .mat file\n",
    "#         sio.savemat(file_path, data_dict)\n",
    "        \n",
    "#         # Print confirmation\n",
    "#         print(f\"Saved {file_name} to {file_path}\")\n",
    "\n",
    "# # Directory where .mat files will be saved (change this path as needed)\n",
    "# save_directory = \"/Users/mirandahulsey-vincent/Desktop/USA5288_song_cell_arrays\"\n",
    "\n",
    "# # Assuming organized_data_frame_with_orders contains the data\n",
    "# generate_and_save_mat_files(organized_data_frame_with_orders, save_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save ALL data into one cell array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import scipy.io as sio\n",
    "# import os\n",
    "\n",
    "# # Function to get the unique dates in the DataFrame\n",
    "# def get_unique_dates(organized_data_frame):\n",
    "#     return sorted(organized_data_frame['Date'].unique())\n",
    "\n",
    "# # Function to generate and save one .mat file with all mapped syllable orders\n",
    "# def generate_and_save_combined_mat_file(organized_data_frame, save_dir):\n",
    "#     # Ensure the save directory exists\n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "#     # Get all unique dates\n",
    "#     unique_dates = get_unique_dates(organized_data_frame)\n",
    "    \n",
    "#     # Get the animal ID from the first row (assuming it's consistent across the dataset)\n",
    "#     animal_id = organized_data_frame['Animal ID'].iloc[0]\n",
    "    \n",
    "#     # Initialize a list to store all mapped syllable orders from each day\n",
    "#     all_mapped_syllable_orders = []\n",
    "    \n",
    "#     # Iterate through each unique date\n",
    "#     for date in unique_dates:\n",
    "#         # Filter the DataFrame for the current date\n",
    "#         day_data = organized_data_frame[organized_data_frame['Date'] == date]\n",
    "        \n",
    "#         # Extract the mapped syllable orders and store them in the list\n",
    "#         mapped_syllable_orders = day_data['mapped_syllable_order'].tolist()\n",
    "        \n",
    "#         # Ensure the mapped_syllable_orders are formatted as a MATLAB cell array (list of lists)\n",
    "#         mapped_syllable_orders_cell = [[order] for order in mapped_syllable_orders]  # Wrap each order in a list\n",
    "        \n",
    "#         # Append the day's syllable orders to the overall list\n",
    "#         all_mapped_syllable_orders.extend(mapped_syllable_orders_cell)\n",
    "    \n",
    "#     # Prepare the data as a dictionary for saving to .mat format\n",
    "#     data_dict = {'all_mapped_syllable_order': all_mapped_syllable_orders}\n",
    "    \n",
    "#     # Generate a file name based on the animal ID\n",
    "#     file_name = f\"{animal_id}_all_mapped_syllable_orders.mat\"\n",
    "#     file_path = os.path.join(save_dir, file_name)\n",
    "    \n",
    "#     # Save the .mat file\n",
    "#     sio.savemat(file_path, data_dict)\n",
    "    \n",
    "#     # Print confirmation\n",
    "#     print(f\"Saved combined syllable orders to {file_name} at {file_path}\")\n",
    "\n",
    "# # Directory where the combined .mat file will be saved (change this path as needed)\n",
    "# save_directory = \"/Users/mirandahulsey-vincent/Desktop/USA5288_song_cell_arrays\"\n",
    "\n",
    "# # Assuming organized_data_frame_with_orders contains the data\n",
    "# generate_and_save_combined_mat_file(organized_data_frame_with_orders, save_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save 2 groups: pre-lesion, and post-lesion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import scipy.io as sio\n",
    "# import os\n",
    "\n",
    "# # Function to get the unique dates in the DataFrame\n",
    "# def get_unique_dates(organized_data_frame):\n",
    "#     return sorted(organized_data_frame['Date'].unique())\n",
    "\n",
    "# # Function to split and save pre-surgery and post-surgery syllable orders\n",
    "# def generate_and_save_pre_post_surgery_mat_files(organized_data_frame, surgery_date, save_dir):\n",
    "#     # Ensure the save directory exists\n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "#     # Get all unique dates\n",
    "#     unique_dates = get_unique_dates(organized_data_frame)\n",
    "    \n",
    "#     # Get the animal ID from the first row (assuming it's consistent across the dataset)\n",
    "#     animal_id = organized_data_frame['Animal ID'].iloc[0]\n",
    "    \n",
    "#     # Initialize lists to store pre-surgery and post-surgery mapped syllable orders\n",
    "#     pre_surgery_orders = []\n",
    "#     post_surgery_orders = []\n",
    "    \n",
    "#     # Iterate through each unique date\n",
    "#     for date in unique_dates:\n",
    "#         # Filter the DataFrame for the current date\n",
    "#         day_data = organized_data_frame[organized_data_frame['Date'] == date]\n",
    "        \n",
    "#         # Extract the mapped syllable orders and store them in a list\n",
    "#         mapped_syllable_orders = day_data['mapped_syllable_order'].tolist()\n",
    "        \n",
    "#         # Ensure the mapped_syllable_orders are formatted as a MATLAB cell array (list of lists)\n",
    "#         mapped_syllable_orders_cell = [[order] for order in mapped_syllable_orders]  # Wrap each order in a list\n",
    "        \n",
    "#         # Compare the current date with the surgery date and store accordingly\n",
    "#         if date < surgery_date:\n",
    "#             pre_surgery_orders.extend(mapped_syllable_orders_cell)\n",
    "#         elif date > surgery_date:\n",
    "#             post_surgery_orders.extend(mapped_syllable_orders_cell)\n",
    "    \n",
    "#     # Prepare the data for saving in .mat format\n",
    "#     data_dict_pre = {'pre_surgery_mapped_syllable_order': pre_surgery_orders}\n",
    "#     data_dict_post = {'post_surgery_mapped_syllable_order': post_surgery_orders}\n",
    "    \n",
    "#     # Generate file names based on the animal ID and pre/post surgery\n",
    "#     file_name_pre = f\"{animal_id}_pre_surgery_mapped_syllable_orders.mat\"\n",
    "#     file_name_post = f\"{animal_id}_post_surgery_mapped_syllable_orders.mat\"\n",
    "    \n",
    "#     # Save the .mat files\n",
    "#     file_path_pre = os.path.join(save_dir, file_name_pre)\n",
    "#     file_path_post = os.path.join(save_dir, file_name_post)\n",
    "    \n",
    "#     sio.savemat(file_path_pre, data_dict_pre)\n",
    "#     sio.savemat(file_path_post, data_dict_post)\n",
    "    \n",
    "#     # Print confirmation\n",
    "#     print(f\"Saved pre-surgery syllable orders to {file_name_pre} at {file_path_pre}\")\n",
    "#     print(f\"Saved post-surgery syllable orders to {file_name_post} at {file_path_post}\")\n",
    "\n",
    "# # Directory where the .mat files will be saved (change this path as needed)\n",
    "# save_directory = \"/Users/mirandahulsey-vincent/Desktop/DECODER_cell_arrays\"\n",
    "\n",
    "# # Define the surgery date (change this to the actual surgery date as needed)\n",
    "# #Format like so: 03.15.2024 MMDDYYY\n",
    "# # surgery_date = '04.30.2024'\n",
    "\n",
    "# # Assuming organized_data_frame_with_orders contains the data\n",
    "# generate_and_save_pre_post_surgery_mat_files(organized_data_frame_with_orders, surgery_date, save_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save into three groups with an equal number of songs in each: \n",
    "1) 3.5k songs post-lesion\n",
    "2) 3.5k songs from immediately pre-lesion\n",
    "3) 3.5k songs from earlier pre-lesion recordings \n",
    "\n",
    "From TG: \"Any comparison should have a balanced number of songs.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, make sure each song contains at least 2 syllables (there will be no syntax trees if you just have songs with one syllabe. Plus this is likely just a trill that's been marked as a song)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>song_present</th>\n",
       "      <th>syllable_onsets_offsets_ms</th>\n",
       "      <th>syllable_onsets_offsets_timebins</th>\n",
       "      <th>Animal ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Minute</th>\n",
       "      <th>Second</th>\n",
       "      <th>syllable_onsets_offsets_ms_dict</th>\n",
       "      <th>syllable_order</th>\n",
       "      <th>mapped_syllable_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USA5337_45370.34258490_3_19_9_30_58.wav</td>\n",
       "      <td>True</td>\n",
       "      <td>{'20': [[0.0, 178.0952380952381], [4028.730158...</td>\n",
       "      <td>{'20': [[0.0, 66], [1493.0, 1527]], '16': [[66...</td>\n",
       "      <td>USA5337</td>\n",
       "      <td>2024-03-19</td>\n",
       "      <td>09</td>\n",
       "      <td>30</td>\n",
       "      <td>58</td>\n",
       "      <td>{'20': [[0.0, 178.0952380952381], [4028.730158...</td>\n",
       "      <td>[20, 21, 19, 5, 16, 4, 20, 12, 28, 26, 28, 26,...</td>\n",
       "      <td>NOLYIXNEVTVTVTVT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>USA5337_45370.40080635_3_19_11_8_0.wav</td>\n",
       "      <td>True</td>\n",
       "      <td>{'20': [[0.0, 129.52380952380952], [4314.76190...</td>\n",
       "      <td>{'20': [[0.0, 48], [1599.0000000000002, 1626]]...</td>\n",
       "      <td>USA5337</td>\n",
       "      <td>2024-03-19</td>\n",
       "      <td>11</td>\n",
       "      <td>08</td>\n",
       "      <td>00</td>\n",
       "      <td>{'20': [[0.0, 129.52380952380952], [4314.76190...</td>\n",
       "      <td>[20, 21, 16, 19, 5, 4, 20, 12, 28, 26, 28, 26,...</td>\n",
       "      <td>NOILYXNEVTVTVTVT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>USA5337_45370.40389621_3_19_11_13_9.wav</td>\n",
       "      <td>True</td>\n",
       "      <td>{'14': [[0.0, 2571.5873015873017]], '28': [[25...</td>\n",
       "      <td>{'14': [[0.0, 953]], '28': [[953.0, 956]], '20...</td>\n",
       "      <td>USA5337</td>\n",
       "      <td>2024-03-19</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>09</td>\n",
       "      <td>{'14': [[0.0, 2571.5873015873017]], '28': [[25...</td>\n",
       "      <td>[14, 28, 20, 7, 0, 7, 0, 7, 0, 7, 0, 1]</td>\n",
       "      <td>GVNaAaAaAaAB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USA5337_45370.31294258_3_19_8_41_34.wav</td>\n",
       "      <td>True</td>\n",
       "      <td>{'20': [[0.0, 172.6984126984127], [3918.095238...</td>\n",
       "      <td>{'20': [[0.0, 64], [1452.0, 1474]], '16': [[64...</td>\n",
       "      <td>USA5337</td>\n",
       "      <td>2024-03-19</td>\n",
       "      <td>08</td>\n",
       "      <td>41</td>\n",
       "      <td>34</td>\n",
       "      <td>{'20': [[0.0, 172.6984126984127], [3918.095238...</td>\n",
       "      <td>[20, 21, 19, 5, 16, 4, 20, 12, 28]</td>\n",
       "      <td>NOLYIXNEV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USA5337_45370.35257293_3_19_9_47_37.wav</td>\n",
       "      <td>True</td>\n",
       "      <td>{'20': [[0.0, 167.30158730158732]], '16': [[16...</td>\n",
       "      <td>{'20': [[0.0, 62]], '16': [[62.0, 503]], '21':...</td>\n",
       "      <td>USA5337</td>\n",
       "      <td>2024-03-19</td>\n",
       "      <td>09</td>\n",
       "      <td>47</td>\n",
       "      <td>37</td>\n",
       "      <td>{'20': [[0.0, 167.30158730158732]], '16': [[16...</td>\n",
       "      <td>[20, 21, 19, 5, 16, 4, 3, 8, 27, 28, 27, 28, 2...</td>\n",
       "      <td>NOLYIXWbUVUVUa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16791</th>\n",
       "      <td>USA5337_45404.59868624_4_22_16_37_48.wav</td>\n",
       "      <td>True</td>\n",
       "      <td>{'20': [[0.0, 240.15873015873018], [3111.26984...</td>\n",
       "      <td>{'20': [[0.0, 89], [1153.0, 1183]], '16': [[89...</td>\n",
       "      <td>USA5337</td>\n",
       "      <td>2024-04-22</td>\n",
       "      <td>16</td>\n",
       "      <td>37</td>\n",
       "      <td>48</td>\n",
       "      <td>{'20': [[0.0, 240.15873015873018], [3111.26984...</td>\n",
       "      <td>[20, 21, 19, 5, 16, 21, 22, 20, 24, 25, 10]</td>\n",
       "      <td>NOLYIOPNRSC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16792</th>\n",
       "      <td>USA5337_45404.32029469_4_22_8_53_49.wav</td>\n",
       "      <td>True</td>\n",
       "      <td>{'20': [[0.0, 240.15873015873018], [4967.77777...</td>\n",
       "      <td>{'20': [[0.0, 89], [1841.0, 1869], [3444.0, 34...</td>\n",
       "      <td>USA5337</td>\n",
       "      <td>2024-04-22</td>\n",
       "      <td>08</td>\n",
       "      <td>53</td>\n",
       "      <td>49</td>\n",
       "      <td>{'20': [[0.0, 240.15873015873018], [4967.77777...</td>\n",
       "      <td>[20, 3, 8, 27, 28, 27, 28, 27, 28, 27, 28, 26,...</td>\n",
       "      <td>NWbUVUVUVUVTVOTVAVBYIXNEVTVTVTVTVTVAPNZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16793</th>\n",
       "      <td>USA5337_45404.67025015_4_22_18_37_5.wav</td>\n",
       "      <td>True</td>\n",
       "      <td>{'20': [[0.0, 218.57142857142858], [2827.93650...</td>\n",
       "      <td>{'20': [[0.0, 81], [1048.0, 1079]], '16': [[81...</td>\n",
       "      <td>USA5337</td>\n",
       "      <td>2024-04-22</td>\n",
       "      <td>18</td>\n",
       "      <td>37</td>\n",
       "      <td>05</td>\n",
       "      <td>{'20': [[0.0, 218.57142857142858], [2827.93650...</td>\n",
       "      <td>[20, 21, 19, 5, 16, 21, 22, 20, 24, 25, 16]</td>\n",
       "      <td>NOLYIOPNRSI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16794</th>\n",
       "      <td>USA5337_45404.60296045_4_22_16_44_56.wav</td>\n",
       "      <td>True</td>\n",
       "      <td>{'20': [[0.0, 218.57142857142858], [2873.80952...</td>\n",
       "      <td>{'20': [[0.0, 81], [1065.0, 1094]], '16': [[81...</td>\n",
       "      <td>USA5337</td>\n",
       "      <td>2024-04-22</td>\n",
       "      <td>16</td>\n",
       "      <td>44</td>\n",
       "      <td>56</td>\n",
       "      <td>{'20': [[0.0, 218.57142857142858], [2873.80952...</td>\n",
       "      <td>[20, 21, 19, 5, 16, 22, 20, 24, 25]</td>\n",
       "      <td>NOLYIPNRS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16795</th>\n",
       "      <td>USA5337_45404.62125791_4_22_17_15_25.wav</td>\n",
       "      <td>True</td>\n",
       "      <td>{'20': [[0.0, 210.47619047619048], [6902.53968...</td>\n",
       "      <td>{'20': [[0.0, 78], [2558.0, 2587]], '16': [[78...</td>\n",
       "      <td>USA5337</td>\n",
       "      <td>2024-04-22</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>25</td>\n",
       "      <td>{'20': [[0.0, 210.47619047619048], [6902.53968...</td>\n",
       "      <td>[20, 21, 19, 5, 16, 4, 9, 2, 21, 22, 20, 23]</td>\n",
       "      <td>NOLYIXcMOPNQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16695 rows  12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      file_name  song_present  \\\n",
       "0       USA5337_45370.34258490_3_19_9_30_58.wav          True   \n",
       "1        USA5337_45370.40080635_3_19_11_8_0.wav          True   \n",
       "2       USA5337_45370.40389621_3_19_11_13_9.wav          True   \n",
       "3       USA5337_45370.31294258_3_19_8_41_34.wav          True   \n",
       "4       USA5337_45370.35257293_3_19_9_47_37.wav          True   \n",
       "...                                         ...           ...   \n",
       "16791  USA5337_45404.59868624_4_22_16_37_48.wav          True   \n",
       "16792   USA5337_45404.32029469_4_22_8_53_49.wav          True   \n",
       "16793   USA5337_45404.67025015_4_22_18_37_5.wav          True   \n",
       "16794  USA5337_45404.60296045_4_22_16_44_56.wav          True   \n",
       "16795  USA5337_45404.62125791_4_22_17_15_25.wav          True   \n",
       "\n",
       "                              syllable_onsets_offsets_ms  \\\n",
       "0      {'20': [[0.0, 178.0952380952381], [4028.730158...   \n",
       "1      {'20': [[0.0, 129.52380952380952], [4314.76190...   \n",
       "2      {'14': [[0.0, 2571.5873015873017]], '28': [[25...   \n",
       "3      {'20': [[0.0, 172.6984126984127], [3918.095238...   \n",
       "4      {'20': [[0.0, 167.30158730158732]], '16': [[16...   \n",
       "...                                                  ...   \n",
       "16791  {'20': [[0.0, 240.15873015873018], [3111.26984...   \n",
       "16792  {'20': [[0.0, 240.15873015873018], [4967.77777...   \n",
       "16793  {'20': [[0.0, 218.57142857142858], [2827.93650...   \n",
       "16794  {'20': [[0.0, 218.57142857142858], [2873.80952...   \n",
       "16795  {'20': [[0.0, 210.47619047619048], [6902.53968...   \n",
       "\n",
       "                        syllable_onsets_offsets_timebins Animal ID       Date  \\\n",
       "0      {'20': [[0.0, 66], [1493.0, 1527]], '16': [[66...   USA5337 2024-03-19   \n",
       "1      {'20': [[0.0, 48], [1599.0000000000002, 1626]]...   USA5337 2024-03-19   \n",
       "2      {'14': [[0.0, 953]], '28': [[953.0, 956]], '20...   USA5337 2024-03-19   \n",
       "3      {'20': [[0.0, 64], [1452.0, 1474]], '16': [[64...   USA5337 2024-03-19   \n",
       "4      {'20': [[0.0, 62]], '16': [[62.0, 503]], '21':...   USA5337 2024-03-19   \n",
       "...                                                  ...       ...        ...   \n",
       "16791  {'20': [[0.0, 89], [1153.0, 1183]], '16': [[89...   USA5337 2024-04-22   \n",
       "16792  {'20': [[0.0, 89], [1841.0, 1869], [3444.0, 34...   USA5337 2024-04-22   \n",
       "16793  {'20': [[0.0, 81], [1048.0, 1079]], '16': [[81...   USA5337 2024-04-22   \n",
       "16794  {'20': [[0.0, 81], [1065.0, 1094]], '16': [[81...   USA5337 2024-04-22   \n",
       "16795  {'20': [[0.0, 78], [2558.0, 2587]], '16': [[78...   USA5337 2024-04-22   \n",
       "\n",
       "      Hour Minute Second                    syllable_onsets_offsets_ms_dict  \\\n",
       "0       09     30     58  {'20': [[0.0, 178.0952380952381], [4028.730158...   \n",
       "1       11     08     00  {'20': [[0.0, 129.52380952380952], [4314.76190...   \n",
       "2       11     13     09  {'14': [[0.0, 2571.5873015873017]], '28': [[25...   \n",
       "3       08     41     34  {'20': [[0.0, 172.6984126984127], [3918.095238...   \n",
       "4       09     47     37  {'20': [[0.0, 167.30158730158732]], '16': [[16...   \n",
       "...    ...    ...    ...                                                ...   \n",
       "16791   16     37     48  {'20': [[0.0, 240.15873015873018], [3111.26984...   \n",
       "16792   08     53     49  {'20': [[0.0, 240.15873015873018], [4967.77777...   \n",
       "16793   18     37     05  {'20': [[0.0, 218.57142857142858], [2827.93650...   \n",
       "16794   16     44     56  {'20': [[0.0, 218.57142857142858], [2873.80952...   \n",
       "16795   17     15     25  {'20': [[0.0, 210.47619047619048], [6902.53968...   \n",
       "\n",
       "                                          syllable_order  \\\n",
       "0      [20, 21, 19, 5, 16, 4, 20, 12, 28, 26, 28, 26,...   \n",
       "1      [20, 21, 16, 19, 5, 4, 20, 12, 28, 26, 28, 26,...   \n",
       "2                [14, 28, 20, 7, 0, 7, 0, 7, 0, 7, 0, 1]   \n",
       "3                     [20, 21, 19, 5, 16, 4, 20, 12, 28]   \n",
       "4      [20, 21, 19, 5, 16, 4, 3, 8, 27, 28, 27, 28, 2...   \n",
       "...                                                  ...   \n",
       "16791        [20, 21, 19, 5, 16, 21, 22, 20, 24, 25, 10]   \n",
       "16792  [20, 3, 8, 27, 28, 27, 28, 27, 28, 27, 28, 26,...   \n",
       "16793        [20, 21, 19, 5, 16, 21, 22, 20, 24, 25, 16]   \n",
       "16794                [20, 21, 19, 5, 16, 22, 20, 24, 25]   \n",
       "16795       [20, 21, 19, 5, 16, 4, 9, 2, 21, 22, 20, 23]   \n",
       "\n",
       "                         mapped_syllable_order  \n",
       "0                             NOLYIXNEVTVTVTVT  \n",
       "1                             NOILYXNEVTVTVTVT  \n",
       "2                                 GVNaAaAaAaAB  \n",
       "3                                    NOLYIXNEV  \n",
       "4                               NOLYIXWbUVUVUa  \n",
       "...                                        ...  \n",
       "16791                              NOLYIOPNRSC  \n",
       "16792  NWbUVUVUVUVTVOTVAVBYIXNEVTVTVTVTVTVAPNZ  \n",
       "16793                              NOLYIOPNRSI  \n",
       "16794                                NOLYIPNRS  \n",
       "16795                             NOLYIXcMOPNQ  \n",
       "\n",
       "[16695 rows x 12 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Define a function to count the number of syllables in each song\n",
    "def count_syllables(row):\n",
    "    # Assuming the syllable order is stored as a list in the 'syllable_order' column\n",
    "    return len(row['syllable_order'])\n",
    "\n",
    "# Apply the function to create a new column with the syllable count\n",
    "organized_data_frame_with_orders['syllable_count'] = organized_data_frame_with_orders.apply(count_syllables, axis=1)\n",
    "\n",
    "# Filter the DataFrame to only include rows with 2 or more syllables\n",
    "filtered_data_frame = organized_data_frame_with_orders[organized_data_frame_with_orders['syllable_count'] >= 2]\n",
    "\n",
    "# Drop the 'syllable_count' column if it's no longer needed\n",
    "filtered_data_frame = filtered_data_frame.drop(columns=['syllable_count'])\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "filtered_data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure out how many songs to include in each group: (!!! Line 25 needs to be updated witht the surgery date!!!)\n",
    "1) Check the number of post-lesion songs, and the number of pre-lesion songs.\n",
    "2) If the number of post-lesion songs is less than 1/2 of the number of pre-lesion songs, use that as the group size.\n",
    "3) If the number of post-lesion songs is greater than 1/2 of the number of pre-lesin songs, use 0.5*the number of pre-lesion soongs as the group size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pre-lesion songs: 13531\n",
      "Number of post-lesion songs: 2735\n",
      "Calculated group size: 2735\n",
      "Number of pre-lesion songs: 13531\n",
      "Number of post-lesion songs: 2735\n",
      "Calculated group size: 2735\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate group size based on pre- and post-lesion song counts\n",
    "def calculate_group_size(organized_data_frame, surgery_date):\n",
    "    # Filter the DataFrame for songs recorded before and after the lesion date\n",
    "    pre_lesion_songs = organized_data_frame[organized_data_frame['Date'] < surgery_date]\n",
    "    post_lesion_songs = organized_data_frame[organized_data_frame['Date'] > surgery_date]\n",
    "    \n",
    "    # Count the number of pre-lesion and post-lesion songs\n",
    "    num_pre_lesion_songs = pre_lesion_songs.shape[0]\n",
    "    num_post_lesion_songs = post_lesion_songs.shape[0]\n",
    "    \n",
    "    # Determine the group size based on the condition\n",
    "    if num_post_lesion_songs < 0.5 * num_pre_lesion_songs:\n",
    "        group_size = num_post_lesion_songs  # Use the number of post-lesion songs as the group size\n",
    "    else:\n",
    "        group_size = int(0.5 * num_pre_lesion_songs)  # Use half the number of pre-lesion songs as the group size\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"Number of pre-lesion songs: {num_pre_lesion_songs}\")\n",
    "    print(f\"Number of post-lesion songs: {num_post_lesion_songs}\")\n",
    "    print(f\"Calculated group size: {group_size}\")\n",
    "    \n",
    "    return group_size\n",
    "\n",
    "# Example usage\n",
    "group_size = calculate_group_size(filtered_data_frame, treatment_date)\n",
    "\n",
    "# Function to calculate group size based on pre- and post-lesion song counts\n",
    "def calculate_group_size(organized_data_frame, surgery_date):\n",
    "    # Filter the DataFrame for songs recorded before and after the lesion date\n",
    "    pre_lesion_songs = organized_data_frame[organized_data_frame['Date'] < surgery_date]\n",
    "    post_lesion_songs = organized_data_frame[organized_data_frame['Date'] > surgery_date]\n",
    "    \n",
    "    # Count the number of pre-lesion and post-lesion songs\n",
    "    num_pre_lesion_songs = pre_lesion_songs.shape[0]\n",
    "    num_post_lesion_songs = post_lesion_songs.shape[0]\n",
    "    \n",
    "    # Determine the group size based on the condition\n",
    "    if num_post_lesion_songs < 0.5 * num_pre_lesion_songs:\n",
    "        group_size = num_post_lesion_songs  # Use the number of post-lesion songs as the group size\n",
    "    else:\n",
    "        group_size = int(0.5 * num_pre_lesion_songs)  # Use half the number of pre-lesion songs as the group size\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"Number of pre-lesion songs: {num_pre_lesion_songs}\")\n",
    "    print(f\"Number of post-lesion songs: {num_post_lesion_songs}\")\n",
    "    print(f\"Calculated group size: {group_size}\")\n",
    "    \n",
    "    return group_size\n",
    "\n",
    "# Example usage\n",
    "group_size = calculate_group_size(filtered_data_frame, treatment_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def filter_surgery_songs(organized_data_frame, surgery_date, group_size):\n",
    "#     # Filter the DataFrame for songs recorded after the lesion date (post-lesion)\n",
    "#     post_lesion_songs = organized_data_frame[organized_data_frame['Date'] > surgery_date]\n",
    "    \n",
    "#     # Select the first 'group_size' number of rows from the post-lesion songs\n",
    "#     filtered_post_surgery_songs = post_lesion_songs.head(group_size)\n",
    "    \n",
    "#     # Filter the DataFrame for songs recorded on or before the lesion date (pre-lesion)\n",
    "#     pre_lesion_songs = organized_data_frame[organized_data_frame['Date'] <= surgery_date]\n",
    "    \n",
    "#     total_pre_lesion_songs = len(pre_lesion_songs)\n",
    "#     print(f\"Total number of pre-lesion songs: {total_pre_lesion_songs}\")\n",
    "    \n",
    "#     if total_pre_lesion_songs < group_size * 2:\n",
    "#         raise ValueError(f\"Not enough pre-lesion songs to split into two groups of size {group_size}. Total available: {total_pre_lesion_songs}\")\n",
    "\n",
    "#     # Select the last 'group_size' number of rows for the latest pre-surgery songs\n",
    "#     latest_pre_surgery_songs = pre_lesion_songs.tail(group_size)\n",
    "    \n",
    "#     # Select the 'group_size' rows just before the latest pre-surgery songs\n",
    "#     earlier_pre_surgery_songs = pre_lesion_songs.iloc[-(group_size * 2):-group_size]\n",
    "    \n",
    "#     print(f\"Number of latest pre-surgery songs: {len(latest_pre_surgery_songs)}\")\n",
    "#     print(f\"Number of earlier pre-surgery songs: {len(earlier_pre_surgery_songs)}\")\n",
    "    \n",
    "#     # Create cell arrays (lists) to hold the mapped syllable orders for each group\n",
    "#     mapped_syllable_orders_post = list(filtered_post_surgery_songs['mapped_syllable_order'])\n",
    "#     mapped_syllable_orders_latest_pre = list(latest_pre_surgery_songs['mapped_syllable_order'])\n",
    "#     mapped_syllable_orders_earlier_pre = list(earlier_pre_surgery_songs['mapped_syllable_order'])\n",
    "\n",
    "#     # Print the results for verification\n",
    "#     print(f\"Filtered {group_size} post-surgery songs' mapped syllable orders:\")\n",
    "#     print(mapped_syllable_orders_post)\n",
    "    \n",
    "#     print(f\"Filtered {group_size} latest pre-surgery songs' mapped syllable orders:\")\n",
    "#     print(mapped_syllable_orders_latest_pre)\n",
    "    \n",
    "#     print(f\"Filtered {group_size} earlier pre-surgery songs' mapped syllable orders:\")\n",
    "#     print(mapped_syllable_orders_earlier_pre)\n",
    "    \n",
    "#     # Return the lists of mapped syllable orders for each group\n",
    "#     return mapped_syllable_orders_post, mapped_syllable_orders_latest_pre, mapped_syllable_orders_earlier_pre\n",
    "\n",
    "# # Example usage\n",
    "# group_size = calculate_group_size(filtered_data_frame, treatment_date)  # Calculate group size\n",
    "# mapped_post, mapped_latest_pre, mapped_earlier_pre = filter_surgery_songs(filtered_data_frame, treatment_date, group_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change line 65 to save the output .mat files to a different folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import scipy.io as sio\n",
    "# import os\n",
    "\n",
    "# # Function to save syllable orders as cell arrays\n",
    "# def save_group_to_mat(mapped_syllable_orders, group_name, animal_id, save_dir):\n",
    "#     # Ensure the save directory exists\n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "#     # Wrap each mapped syllable order in a list (for MATLAB cell array compatibility)\n",
    "#     mapped_syllable_orders_cell = [[order] for order in mapped_syllable_orders]\n",
    "    \n",
    "#     # Prepare the data as a dictionary for saving to .mat format\n",
    "#     data_dict = {f'mapped_syllable_order_{group_name}': mapped_syllable_orders_cell}\n",
    "    \n",
    "#     # Generate a file name based on the animal ID and group name\n",
    "#     file_name = f\"{animal_id}_{group_name}.mat\"\n",
    "#     file_path = os.path.join(save_dir, file_name)\n",
    "    \n",
    "#     # Save the .mat file\n",
    "#     sio.savemat(file_path, data_dict)\n",
    "    \n",
    "#     # Print confirmation\n",
    "#     print(f\"Saved {file_name} to {file_path}\")\n",
    "\n",
    "# # Function to process and save pre- and post-surgery syllable orders\n",
    "# def filter_surgery_songs(organized_data_frame, surgery_date, group_size, save_dir):\n",
    "#     # Filter the DataFrame for songs recorded after the lesion date (post-lesion)\n",
    "#     post_lesion_songs = organized_data_frame[organized_data_frame['Date'] > surgery_date]\n",
    "    \n",
    "#     # Select the first 'group_size' number of rows from the post-lesion songs\n",
    "#     filtered_post_surgery_songs = post_lesion_songs.head(group_size)\n",
    "    \n",
    "#     # Filter the DataFrame for songs recorded on or before the lesion date (pre-lesion)\n",
    "#     pre_lesion_songs = organized_data_frame[organized_data_frame['Date'] <= surgery_date]\n",
    "    \n",
    "#     total_pre_lesion_songs = len(pre_lesion_songs)\n",
    "#     print(f\"Total number of pre-lesion songs: {total_pre_lesion_songs}\")\n",
    "    \n",
    "#     if total_pre_lesion_songs < group_size * 2:\n",
    "#         raise ValueError(f\"Not enough pre-lesion songs to split into two groups of size {group_size}. Total available: {total_pre_lesion_songs}\")\n",
    "\n",
    "#     # Select the last 'group_size' number of rows for the latest pre-surgery songs\n",
    "#     latest_pre_surgery_songs = pre_lesion_songs.tail(group_size)\n",
    "    \n",
    "#     # Select the 'group_size' rows just before the latest pre-surgery songs\n",
    "#     earlier_pre_surgery_songs = pre_lesion_songs.iloc[-(group_size * 2):-group_size]\n",
    "    \n",
    "#     print(f\"Number of latest pre-surgery songs: {len(latest_pre_surgery_songs)}\")\n",
    "#     print(f\"Number of earlier pre-surgery songs: {len(earlier_pre_surgery_songs)}\")\n",
    "    \n",
    "#     # Create lists to hold the mapped syllable orders for each group\n",
    "#     mapped_syllable_orders_post = list(filtered_post_surgery_songs['mapped_syllable_order'])\n",
    "#     mapped_syllable_orders_latest_pre = list(latest_pre_surgery_songs['mapped_syllable_order'])\n",
    "#     mapped_syllable_orders_earlier_pre = list(earlier_pre_surgery_songs['mapped_syllable_order'])\n",
    "\n",
    "#     # Get the animal ID (assuming it's consistent across the dataset)\n",
    "#     animal_id = organized_data_frame['Animal ID'].iloc[0]\n",
    "    \n",
    "#     # Save each group of syllable orders into a separate .mat file\n",
    "#     save_group_to_mat(mapped_syllable_orders_post, 'post_lesion_syllable_orders', animal_id, save_dir)\n",
    "#     save_group_to_mat(mapped_syllable_orders_latest_pre, 'latest_pre_lesion_syllable_orders', animal_id, save_dir)\n",
    "#     save_group_to_mat(mapped_syllable_orders_earlier_pre, 'earlier_pre_lesion_syllable_orders', animal_id, save_dir)\n",
    "\n",
    "# save_directory = \"/Users/mirandahulsey-vincent/Desktop/DECODER_cell_arrays\"\n",
    "# # Assuming organized_data_frame_with_orders contains the data\n",
    "# filter_surgery_songs(organized_data_frame_with_orders, treatment_date, group_size, save_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW code to split  the data into early pre-lesion, late pre-lesion, early post-lesion, late post-lesion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of pre-lesion songs: 14051\n",
      "Total number of post-lesion songs: 2745\n",
      "Calculated group size: 1372\n",
      "Saved USA5337_early_pre_lesion_syllable_orders.mat to /Users/mirandahulsey-vincent/Desktop/DECODER_cell_arrays/USA5337_early_pre_lesion_syllable_orders.mat\n",
      "Saved USA5337_late_pre_lesion_syllable_orders.mat to /Users/mirandahulsey-vincent/Desktop/DECODER_cell_arrays/USA5337_late_pre_lesion_syllable_orders.mat\n",
      "Saved USA5337_early_post_lesion_syllable_orders.mat to /Users/mirandahulsey-vincent/Desktop/DECODER_cell_arrays/USA5337_early_post_lesion_syllable_orders.mat\n",
      "Saved USA5337_late_post_lesion_syllable_orders.mat to /Users/mirandahulsey-vincent/Desktop/DECODER_cell_arrays/USA5337_late_post_lesion_syllable_orders.mat\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "import os\n",
    "\n",
    "# Function to save syllable orders as cell arrays\n",
    "def save_group_to_mat(mapped_syllable_orders, group_name, animal_id, save_dir):\n",
    "    # Ensure the save directory exists\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Wrap each mapped syllable order in a list (for MATLAB cell array compatibility)\n",
    "    mapped_syllable_orders_cell = [[order] for order in mapped_syllable_orders]\n",
    "    \n",
    "    # Prepare the data as a dictionary for saving to .mat format\n",
    "    data_dict = {f'mapped_syllable_order_{group_name}': mapped_syllable_orders_cell}\n",
    "    \n",
    "    # Generate a file name based on the animal ID and group name\n",
    "    file_name = f\"{animal_id}_{group_name}.mat\"\n",
    "    file_path = os.path.join(save_dir, file_name)\n",
    "    \n",
    "    # Save the .mat file\n",
    "    sio.savemat(file_path, data_dict)\n",
    "    \n",
    "    # Print confirmation\n",
    "    print(f\"Saved {file_name} to {file_path}\")\n",
    "\n",
    "# Function to process and save pre- and post-surgery syllable orders\n",
    "def filter_surgery_songs(organized_data_frame, surgery_date, save_dir):\n",
    "    # Filter the DataFrame for songs recorded after and before the lesion date\n",
    "    pre_lesion_songs = organized_data_frame[organized_data_frame['Date'] <= surgery_date]\n",
    "    post_lesion_songs = organized_data_frame[organized_data_frame['Date'] > surgery_date]\n",
    "\n",
    "    # Count the total number of songs\n",
    "    num_pre_lesion_songs = len(pre_lesion_songs)\n",
    "    num_post_lesion_songs = len(post_lesion_songs)\n",
    "\n",
    "    print(f\"Total number of pre-lesion songs: {num_pre_lesion_songs}\")\n",
    "    print(f\"Total number of post-lesion songs: {num_post_lesion_songs}\")\n",
    "\n",
    "    # Determine the group size\n",
    "    group_size = min(num_pre_lesion_songs // 2, num_post_lesion_songs // 2)\n",
    "\n",
    "    if group_size == 0:\n",
    "        raise ValueError(\"Not enough songs to split into groups.\")\n",
    "\n",
    "    print(f\"Calculated group size: {group_size}\")\n",
    "\n",
    "    # Split pre-lesion songs into early and late groups\n",
    "    early_pre_lesion_songs = pre_lesion_songs.head(group_size)\n",
    "    late_pre_lesion_songs = pre_lesion_songs.tail(group_size)\n",
    "\n",
    "    # Split post-lesion songs into early and late groups\n",
    "    early_post_lesion_songs = post_lesion_songs.head(group_size)\n",
    "    late_post_lesion_songs = post_lesion_songs.tail(group_size)\n",
    "\n",
    "    # Extract mapped syllable orders for each group\n",
    "    mapped_early_pre = list(early_pre_lesion_songs['mapped_syllable_order'])\n",
    "    mapped_late_pre = list(late_pre_lesion_songs['mapped_syllable_order'])\n",
    "    mapped_early_post = list(early_post_lesion_songs['mapped_syllable_order'])\n",
    "    mapped_late_post = list(late_post_lesion_songs['mapped_syllable_order'])\n",
    "\n",
    "    # Get the animal ID (assuming it's consistent across the dataset)\n",
    "    animal_id = organized_data_frame['Animal ID'].iloc[0]\n",
    "\n",
    "    # Save each group of syllable orders into a separate .mat file\n",
    "    save_group_to_mat(mapped_early_pre, 'early_pre_lesion_syllable_orders', animal_id, save_dir)\n",
    "    save_group_to_mat(mapped_late_pre, 'late_pre_lesion_syllable_orders', animal_id, save_dir)\n",
    "    save_group_to_mat(mapped_early_post, 'early_post_lesion_syllable_orders', animal_id, save_dir)\n",
    "    save_group_to_mat(mapped_late_post, 'late_post_lesion_syllable_orders', animal_id, save_dir)\n",
    "\n",
    "# Example usage\n",
    "save_directory = \"/Users/mirandahulsey-vincent/Desktop/DECODER_cell_arrays\"\n",
    "filter_surgery_songs(organized_data_frame_with_orders, treatment_date, save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syntax_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
